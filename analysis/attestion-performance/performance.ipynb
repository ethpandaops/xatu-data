{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%pip install jupysql clickhouse_sqlalchemy matplotlib python-dotenv pandas seaborn imageio plotly polars-u64-idx > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import imageio\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 300  # Set high DPI for better quality figures\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Use a nicer plotly theme\n",
    "pio.templates.default = \"seaborn\"  # Clean, professional theme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Config:\n",
    "    def __init__(self, time_ranges, network):\n",
    "        self.time_ranges = time_ranges\n",
    "        self.network = network\n",
    "\n",
    "config = Config(\n",
    "    [\n",
    "        # (\"2025-03-28T12:10:00Z\", \"2025-03-28T13:10:00Z\"),\n",
    "        # (\"2025-04-01T05:10:00Z\", \"2025-04-01T06:10:00Z\")\n",
    "\n",
    "        # Before/After Fork, with Nimbus running latest\n",
    "        # (\"2025-03-23T05:10:00Z\", \"2025-03-23T05:20:00Z\"),\n",
    "        # (\"2025-04-01T05:10:00Z\", \"2025-04-01T05:20:00Z\")\n",
    "\n",
    "        # Before/After Fork, with Nimbus running old version\n",
    "        # (\"2025-03-23T05:10:00Z\", \"2025-03-23T06:10:00Z\"),\n",
    "        # (\"2025-04-01T05:10:00Z\", \"2025-04-01T06:10:00Z\")\n",
    "\n",
    "        # Before/After Fork, with Nimbus running old version\n",
    "        # (\"2025-03-23T05:10:00Z\", \"2025-03-23T08:10:00Z\"),\n",
    "        # (\"2025-03-24T05:10:00Z\", \"2025-03-24T08:10:00Z\"),\n",
    "        # (\"2025-03-25T05:10:00Z\", \"2025-03-25T08:10:00Z\"),\n",
    "        # (\"2025-03-26T05:10:00Z\", \"2025-03-26T08:10:00Z\"),\n",
    "        # (\"2025-03-27T05:10:00Z\", \"2025-03-27T08:10:00Z\"),\n",
    "        # (\"2025-03-28T05:10:00Z\", \"2025-03-28T08:10:00Z\"),\n",
    "        # (\"2025-03-29T05:10:00Z\", \"2025-03-29T08:10:00Z\"),\n",
    "        # (\"2025-03-30T05:10:00Z\", \"2025-03-30T08:10:00Z\"),\n",
    "        # (\"2025-03-31T05:10:00Z\", \"2025-03-31T08:10:00Z\"),\n",
    "        # (\"2025-04-01T05:10:00Z\", \"2025-04-01T08:10:00Z\"),\n",
    "        # (\"2025-04-02T05:10:00Z\", \"2025-04-02T08:10:00Z\"),\n",
    "\n",
    "        # (\"2025-03-17T05:10:00Z\", \"2025-03-17T05:30:00Z\"),\n",
    "        (\"2025-03-18T05:10:00Z\", \"2025-03-18T05:30:00Z\"),\n",
    "        (\"2025-03-19T05:10:00Z\", \"2025-03-19T05:30:00Z\"),\n",
    "        (\"2025-03-20T05:10:00Z\", \"2025-03-20T05:30:00Z\"),\n",
    "        (\"2025-03-21T05:10:00Z\", \"2025-03-21T05:30:00Z\"),\n",
    "        (\"2025-03-22T05:10:00Z\", \"2025-03-22T05:30:00Z\"),\n",
    "        (\"2025-03-23T05:10:00Z\", \"2025-03-23T05:30:00Z\"),\n",
    "        (\"2025-03-24T05:10:00Z\", \"2025-03-24T05:30:00Z\"),\n",
    "        (\"2025-03-25T05:10:00Z\", \"2025-03-25T05:30:00Z\"),\n",
    "        (\"2025-03-26T05:10:00Z\", \"2025-03-26T05:30:00Z\"),\n",
    "        (\"2025-03-27T05:10:00Z\", \"2025-03-27T05:30:00Z\"),\n",
    "        (\"2025-03-28T05:10:00Z\", \"2025-03-28T05:30:00Z\"),\n",
    "        (\"2025-03-29T05:10:00Z\", \"2025-03-29T05:30:00Z\"),\n",
    "        (\"2025-03-30T05:10:00Z\", \"2025-03-30T05:30:00Z\"),\n",
    "        (\"2025-03-31T05:10:00Z\", \"2025-03-31T05:30:00Z\"),\n",
    "        (\"2025-04-01T05:10:00Z\", \"2025-04-01T05:30:00Z\"),\n",
    "        (\"2025-04-02T05:10:00Z\", \"2025-04-02T05:30:00Z\"),\n",
    "    ],\n",
    "    \"hoodi\"\n",
    ")\n",
    "\n",
    "# Hoodi Electra Fork\n",
    "event_date = pd.to_datetime(\"2025-03-27T00:37:12Z\", utc=True)\n",
    "\n",
    "# config = Config(\n",
    "#     [\n",
    "#         # (\"2025-03-25T12:10:00Z\", \"2025-03-25T12:30:00Z\"),\n",
    "#         # (\"2025-03-27T12:10:00Z\", \"2025-03-27T12:30:00Z\")\n",
    "#         (\"2025-03-2T12:10:00Z\", \"2025-03-25T12:30:00Z\"),\n",
    "#         (\"2025-03-27T12:10:00Z\", \"2025-03-27T12:30:00Z\")\n",
    "#     ],\n",
    "#     \"hoodi\"\n",
    "# )\n",
    "\n",
    "# event_date = pd.to_datetime(\"2025-03-26T14:37:12Z\", utc=True)\n",
    "event_date_naive = event_date.tz_localize(None) if event_date.tzinfo is not None else event_date\n",
    "\n",
    "annotations = {\n",
    "    \"2025-03-26T14:37:12Z\": \"Hoodi Electra Fork\",\n",
    "}\n",
    "\n",
    "# Convert annotation timestamps to datetime objects for later use\n",
    "annotation_datetimes = {\n",
    "    datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\")): msg\n",
    "    for ts, msg in annotations.items()\n",
    "}\n",
    "\n",
    "# Print the config contents\n",
    "print(\"config network:\", config.network)\n",
    "print(\"config time ranges:\", config.time_ranges)\n",
    "print(\"event date:\", event_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ClickHouse\n",
    "import os\n",
    "username = os.getenv('XATU_CLICKHOUSE_USERNAME')\n",
    "password = os.getenv('XATU_CLICKHOUSE_PASSWORD')\n",
    "host = os.getenv('XATU_CLICKHOUSE_HOST')\n",
    "\n",
    "\n",
    "db_url = f\"clickhouse+http://{username}:{password}@{host}:443/default?protocol=https\"\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load known validators from the YAML file\n",
    "import yaml\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def load_validators(network):\n",
    "    \"\"\"Load validators from the YAML file for the specified network.\"\"\"\n",
    "    validators_path = Path(f\"../../assets/ethereum/{network}/validators.yaml\")\n",
    "    \n",
    "    if not validators_path.exists():\n",
    "        print(f\"Validators file not found at {validators_path}\")\n",
    "        return {}\n",
    "    \n",
    "    with open(validators_path, 'r') as file:\n",
    "        validators_data = yaml.safe_load(file)\n",
    "    \n",
    "    # Process the validators data\n",
    "    validators_map = {}\n",
    "    for range_str, client in validators_data.items():\n",
    "        if isinstance(range_str, str) and '-' in range_str:\n",
    "            start, end = map(int, range_str.split('-'))\n",
    "            for validator_index in range(start, end + 1):\n",
    "                validators_map[validator_index] = client\n",
    "        elif isinstance(range_str, int):\n",
    "            # Handle single validator case\n",
    "            validators_map[range_str] = client\n",
    "    \n",
    "    return validators_map\n",
    "\n",
    "def get_validator_entity(validator_index, validators_map):\n",
    "    \"\"\"Get the entity/client associated with a validator index.\"\"\"\n",
    "    if validator_index in validators_map:\n",
    "        return validators_map[validator_index]\n",
    "    return \"unknown\"\n",
    "\n",
    "def get_validator_entities(validator_indices, validators_map):\n",
    "    \"\"\"Get entities for multiple validator indices.\"\"\"\n",
    "    return {idx: get_validator_entity(idx, validators_map) for idx in validator_indices}\n",
    "\n",
    "def is_validator_from_entity(validator_index, entity, validators_map):\n",
    "    \"\"\"Check if a validator belongs to a specific entity.\"\"\"\n",
    "    return get_validator_entity(validator_index, validators_map) == entity\n",
    "\n",
    "def get_validator_entity_sizes(validators_map):\n",
    "    \"\"\"Get the size of each entity.\"\"\"\n",
    "    return {entity: len(validators_map[entity]) for entity in validators_map}\n",
    "\n",
    "# Load validators for the configured network\n",
    "validators = load_validators(config.network)\n",
    "print(f\"Loaded {len(validators)} validators for {config.network}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be loading in all the `elaborated_attestations` from the `canonical_beacon_elab_attestations` table\n",
    "# canonical_beacon_elaborated_attestation\n",
    "# #\n",
    "# Contains elaborated attestations from beacon blocks.\n",
    "\n",
    "# Availability\n",
    "# #\n",
    "# Data is partitioned daily on slot_start_date_time for the following networks:\n",
    "\n",
    "# mainnet: 2020-12-01 to 2025-03-10\n",
    "# holesky: 2023-09-23 to 2025-03-11\n",
    "# sepolia: 2022-06-22 to 2025-03-10\n",
    "# Examples\n",
    "# #\n",
    "# Parquet file\n",
    "# Your Clickhouse\n",
    "# EthPandaOps Clickhouse\n",
    "# Columns\n",
    "# #\n",
    "# Name\tType\tDescription\n",
    "# updated_date_time\tDateTime\tWhen this row was last updated\n",
    "# block_slot\tUInt32\tThe slot number of the block containing the attestation\n",
    "# block_slot_start_date_time\tDateTime\tThe wall clock time when the block slot started\n",
    "# block_epoch\tUInt32\tThe epoch number of the block containing the attestation\n",
    "# block_epoch_start_date_time\tDateTime\tThe wall clock time when the block epoch started\n",
    "# position_in_block\tUInt32\tThe position of the attestation in the block\n",
    "# block_root\tFixedString(66)\tThe root of the block containing the attestation\n",
    "# validators\tArray(UInt32)\tArray of validator indices participating in the attestation\n",
    "# committee_index\tLowCardinality(String)\tThe index of the committee making the attestation\n",
    "# beacon_block_root\tFixedString(66)\tThe root of the beacon block being attested to\n",
    "# slot\tUInt32\tThe slot number being attested to\n",
    "# slot_start_date_time\tDateTime\t**\n",
    "# epoch\tUInt32\t**\n",
    "# epoch_start_date_time\tDateTime\t**\n",
    "# source_epoch\tUInt32\tThe source epoch referenced in the attestation\n",
    "# source_epoch_start_date_time\tDateTime\tThe wall clock time when the source epoch started\n",
    "# source_root\tFixedString(66)\tThe root of the source checkpoint in the attestation\n",
    "# target_epoch\tUInt32\tThe target epoch referenced in the attestation\n",
    "# target_epoch_start_date_time\tDateTime\tThe wall clock time when the target epoch started\n",
    "# target_root\tFixedString(66)\tThe root of the target checkpoint in the attestation\n",
    "# meta_client_name\tLowCardinality(String)\tName of the client that generated the event\n",
    "# meta_client_id\tString\tUnique Session ID of the client that generated the event. This changes every time the client is restarted.\n",
    "# meta_client_version\tLowCardinality(String)\tVersion of the client that generated the event\n",
    "# meta_client_implementation\tLowCardinality(String)\tImplementation of the client that generated the event\n",
    "# meta_client_os\tLowCardinality(String)\tOperating system of the client that generated the event\n",
    "# meta_client_ip\tNullable(IPv6)\tIP address of the client that generated the event\n",
    "# meta_client_geo_city\tLowCardinality(String)\tCity of the client that generated the event\n",
    "# meta_client_geo_country\tLowCardinality(String)\tCountry of the client that generated the event\n",
    "# meta_client_geo_country_code\tLowCardinality(String)\tCountry code of the client that generated the event\n",
    "# meta_client_geo_continent_code\tLowCardinality(String)\tContinent code of the client that generated the event\n",
    "# meta_client_geo_longitude\tNullable(Float64)\tLongitude of the client that generated the event\n",
    "# meta_client_geo_latitude\tNullable(Float64)\tLatitude of the client that generated the event\n",
    "# meta_client_geo_autonomous_system_number\tNullable(UInt32)\tAutonomous system number of the client that generated the event\n",
    "# meta_client_geo_autonomous_system_organization\tNullable(String)\tAutonomous system organization of the client that generated the event\n",
    "# meta_network_id\tInt32\tEthereum network ID\n",
    "# meta_network_name\tLowCardinality(String)\tEthereum network name\n",
    "# meta_consensus_version\tLowCardinality(String)\tEthereum consensus client version that generated the event\n",
    "# meta_consensus_version_major\tLowCardinality(String)\tEthereum consensus client major version that generated the event\n",
    "# meta_consensus_version_minor\tLowCardinality(String)\tEthereum consensus client minor version that generated the event\n",
    "# meta_consensus_version_patch\tLowCardinality(String)\tEthereum consensus client patch version that generated the event\n",
    "# meta_consensus_implementation\tLowCardinality(String)\tEthereum consensus client implementation that generated the event\n",
    "# meta_labels\tMap(String, String)\tLabels associated with the event\n",
    "\n",
    "import polars as pl\n",
    "import concurrent.futures\n",
    "from sqlalchemy import text # Make sure text is imported if not already\n",
    "\n",
    "def fetch_attestations_for_time_range(time_range, network):\n",
    "    start_date, end_date = time_range\n",
    "    print(f\"Loading attestations for time range: {start_date} to {end_date} for network: {network}\")\n",
    "    \n",
    "    attestations_query = text(\"\"\"\n",
    "        SELECT\n",
    "            beacon_block_root,\n",
    "            block_slot,\n",
    "            block_slot_start_date_time,\n",
    "            committee_index,\n",
    "            slot,\n",
    "            slot_start_date_time,\n",
    "            epoch,\n",
    "            position_in_block,\n",
    "            validator\n",
    "        FROM (\n",
    "            SELECT\n",
    "                beacon_block_root,\n",
    "                block_slot,\n",
    "                block_slot_start_date_time,\n",
    "                committee_index,\n",
    "                slot,\n",
    "                slot_start_date_time,\n",
    "                epoch,\n",
    "                position_in_block,\n",
    "                validator,\n",
    "                min(block_slot) OVER (PARTITION BY validator, epoch) as first_block_slot\n",
    "            FROM (\n",
    "                SELECT\n",
    "                    beacon_block_root,\n",
    "                    block_slot,\n",
    "                    block_slot_start_date_time,\n",
    "                    committee_index,\n",
    "                    slot,\n",
    "                    slot_start_date_time,\n",
    "                    epoch,\n",
    "                    position_in_block,\n",
    "                    validator\n",
    "                FROM canonical_beacon_elaborated_attestation\n",
    "                ARRAY JOIN validators AS validator\n",
    "                WHERE\n",
    "                    block_epoch_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "                    AND meta_network_name = :network\n",
    "            )\n",
    "        )\n",
    "        WHERE block_slot = first_block_slot\n",
    "        ORDER BY block_slot ASC, position_in_block ASC, validator ASC\n",
    "    \"\"\")\n",
    "\n",
    "    # Assuming 'connection' is defined globally or passed appropriately\n",
    "    result = connection.execute(attestations_query, {\n",
    "        \"start_date\": start_date.replace('Z', ''), \n",
    "        \"end_date\": end_date.replace('Z', ''), \n",
    "        \"network\": network\n",
    "    })\n",
    "    \n",
    "    data = result.fetchall()\n",
    "    \n",
    "    # Define the schema explicitly using Polars dtypes\n",
    "    schema = {\n",
    "        'beacon_block_root': pl.Utf8, \n",
    "        'block_slot': pl.UInt32, \n",
    "        'block_slot_start_date_time': pl.Datetime, \n",
    "        'committee_index': pl.Utf8, # Or pl.Categorical if preferred\n",
    "        'slot': pl.UInt32, \n",
    "        'slot_start_date_time': pl.Datetime,\n",
    "        'epoch': pl.UInt32, \n",
    "        'position_in_block': pl.UInt32, \n",
    "        'validator': pl.UInt32\n",
    "    }\n",
    "    column_names = list(schema.keys())\n",
    "\n",
    "    # Create DataFrame with explicit schema\n",
    "    # Polars can infer schema from data, but explicit is often better.\n",
    "    # If data is empty, this creates an empty DataFrame with the correct columns and types.\n",
    "    df = pl.DataFrame(data, schema=schema)\n",
    "    \n",
    "    print(f\"Loaded {len(df)} attestations from time window {start_date} to {end_date}\")\n",
    "    return df\n",
    "\n",
    "# Load all elaborated attestations for the time period in parallel\n",
    "# Assuming 'config' and 'validators' are defined globally or passed appropriately\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [executor.submit(fetch_attestations_for_time_range, time_range, config.network) \n",
    "               for time_range in config.time_ranges]\n",
    "    \n",
    "    attestation_dfs = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "# Combine all dataframes\n",
    "all_attestations = pl.concat(attestation_dfs) if attestation_dfs else pl.DataFrame({name: pl.Series(name=name, dtype=dtype) for name, dtype in schema.items()})\n",
    "\n",
    "\n",
    "# Add validator entity information only if there are attestations\n",
    "if not all_attestations.is_empty():\n",
    "    validators_dict = {k: v for k, v in validators.items()} # Assuming 'validators' is loaded\n",
    "    all_attestations = all_attestations.with_columns(\n",
    "        pl.col('validator').map_elements(lambda x: get_validator_entity(x, validators_dict), return_dtype=pl.Utf8).alias('entity')\n",
    "    )\n",
    "    print(f\"Loaded {len(all_attestations)} attestations for {all_attestations.select(pl.col('validator').n_unique())[0, 0]} unique validators\")\n",
    "else:\n",
    "    # Ensure the 'entity' column exists even if empty\n",
    "    all_attestations = all_attestations.with_columns(pl.lit(None, dtype=pl.Utf8).alias('entity'))\n",
    "    print(\"Loaded 0 attestations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"Attestation Performance Per Slot\": {\n",
    "        \"description\": (),\n",
    "        \"metrics\": {\n",
    "            \"correct_head_percentage\": \"The percentage of attestations that have the correct head.\",\n",
    "        }\n",
    "    },\n",
    "    \"Attestation Performance Per Attestation\": {\n",
    "        \"description\": (),\n",
    "        \"metrics\": {\n",
    "            \"head_correct\": \"Whether the attestation has the correct head.\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_full_metric_description(metric_name):\n",
    "    \"\"\"\n",
    "    Combines the top-level description with the specific metric description.\n",
    "    \n",
    "    Args:\n",
    "        metric_name: The name of the metric to get the description for\n",
    "        \n",
    "    Returns:\n",
    "        A combined description string or None if metric not found\n",
    "    \"\"\"\n",
    "    for category, data in metrics.items():\n",
    "        if metric_name in data[\"metrics\"]:\n",
    "            return f\"{data['description']} {data['metrics'][metric_name]}\"\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sqlalchemy import text\n",
    "import pandas as pd # Keep pandas for date manipulation if needed\n",
    "\n",
    "# Assume 'all_attestations' is already a Polars DataFrame from the previous cell\n",
    "# Assume 'validators' is a dictionary {validator_index: entity_name}\n",
    "# Assume 'connection' is a valid SQLAlchemy connection\n",
    "# Assume 'config' has 'time_ranges' and 'network'\n",
    "# Assume 'event_date' is defined, e.g., event_date = \"2025-03-28T00:00:00Z\"\n",
    "# Assume 'event_date_naive' is defined (datetime without tzinfo)\n",
    "\n",
    "# Assign all_attestations directly as it's already a Polars DataFrame\n",
    "all_attestations_pl = all_attestations\n",
    "print(f\"Using existing Polars DataFrame with {len(all_attestations_pl)} attestations.\")\n",
    "\n",
    "# Convert validators map to a Polars DataFrame for joining\n",
    "print(\"Creating validators Polars DataFrame...\")\n",
    "try:\n",
    "    validators_df = pl.DataFrame({\n",
    "        'validator': [int(k) for k in validators.keys()],\n",
    "        'entity': list(validators.values())\n",
    "    }).with_columns(pl.col('validator').cast(pl.UInt32)) # Match attestation validator type if needed\n",
    "    print(f\"Validators DataFrame created with {len(validators_df)} entries.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating validators DataFrame: {e}. Ensure validator keys are numeric.\")\n",
    "    # Handle error or create an empty DF\n",
    "    validators_df = pl.DataFrame({'validator': pl.Series([], dtype=pl.UInt32), 'entity': pl.Series([], dtype=pl.Utf8)})\n",
    "\n",
    "\n",
    "# Fetch beacon blocks using Polars\n",
    "def fetch_beacon_blocks_pl(time_ranges, network, connection):\n",
    "    all_beacon_blocks_pl = []\n",
    "    for start_date, end_date in time_ranges:\n",
    "        # Using pandas for date padding for simplicity\n",
    "        padded_start_date = (pd.to_datetime(start_date) - pd.Timedelta(hours=1)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "        padded_end_date = (pd.to_datetime(end_date) + pd.Timedelta(hours=1)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "        beacon_blocks_query = text(\"\"\"\n",
    "            SELECT\n",
    "                slot,\n",
    "                block_root,\n",
    "                proposer_index,\n",
    "                slot_start_date_time -- Need this for period calculation later\n",
    "            FROM canonical_beacon_block FINAL\n",
    "            WHERE\n",
    "                slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "                AND meta_network_name = :network\n",
    "            ORDER BY slot ASC\n",
    "        \"\"\")\n",
    "\n",
    "        try:\n",
    "            # Using pandas read_sql as a bridge\n",
    "            # TODO: Explore direct Polars read_sql if connector supports it well\n",
    "            beacon_blocks_pd = pd.read_sql(\n",
    "                beacon_blocks_query,\n",
    "                connection.engine, # Assuming connection has an engine attribute\n",
    "                params={\"start_date\": padded_start_date.replace('Z', ''), \"end_date\": padded_end_date.replace('Z', ''), \"network\": network}\n",
    "            )\n",
    "            # Convert timestamp to Polars Datetime with UTC timezone explicitly\n",
    "            beacon_blocks = pl.from_pandas(beacon_blocks_pd).with_columns(\n",
    "                pl.col(\"slot_start_date_time\").dt.replace_time_zone(\"UTC\")\n",
    "            )\n",
    "            all_beacon_blocks_pl.append(beacon_blocks)\n",
    "            print(f\"Fetched {len(beacon_blocks)} beacon blocks for range {start_date} to {end_date}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error fetching beacon blocks for range {start_date} to {end_date}: {e}\")\n",
    "\n",
    "\n",
    "    if not all_beacon_blocks_pl:\n",
    "        # Return an empty DataFrame with the correct schema if no data was fetched\n",
    "        return pl.DataFrame({\n",
    "            'slot': pl.Series([], dtype=pl.UInt64),\n",
    "            'block_root': pl.Series([], dtype=pl.Utf8),\n",
    "            'proposer_index': pl.Series([], dtype=pl.UInt64),\n",
    "            'slot_start_date_time': pl.Series([], dtype=pl.Datetime(time_unit=\"us\", time_zone=\"UTC\")) # Match type\n",
    "        })\n",
    "\n",
    "    return pl.concat(all_beacon_blocks_pl)\n",
    "\n",
    "print(\"Fetching beacon blocks...\")\n",
    "beacon_blocks_pl = fetch_beacon_blocks_pl(config.time_ranges, config.network, connection)\n",
    "\n",
    "# Cast proposer_index if necessary (adjust type based on actual data)\n",
    "beacon_blocks_pl = beacon_blocks_pl.with_columns(\n",
    "    pl.col(\"proposer_index\").cast(pl.UInt32) # Match validator index type\n",
    ")\n",
    "\n",
    "# Add proposer entity by joining with validators_df\n",
    "# Rename columns to distinguish proposer info\n",
    "beacon_blocks_pl = beacon_blocks_pl.join(\n",
    "    validators_df.rename({\"validator\": \"proposer_index\", \"entity\": \"proposer_entity\"}),\n",
    "    on=\"proposer_index\",\n",
    "    how=\"left\"\n",
    ")\n",
    "# Set unknown entities to 'unknown'\n",
    "beacon_blocks_pl = beacon_blocks_pl.with_columns(\n",
    "    pl.col(\"proposer_entity\").fill_null(\"unknown\")\n",
    ")\n",
    "print(f\"Beacon blocks DataFrame created with {len(beacon_blocks_pl)} entries.\")\n",
    "print(beacon_blocks_pl.head())\n",
    "\n",
    "\n",
    "# Prepare canonical block roots DataFrame for joining\n",
    "canonical_blocks_df = beacon_blocks_pl.select(\n",
    "    pl.col(\"slot\").alias(\"canonical_slot\"), # Alias to avoid name collision if joining on 'slot' later\n",
    "    pl.col(\"block_root\").alias(\"canonical_block_root\")\n",
    ")\n",
    "print(f\"Canonical block roots DataFrame created with {len(canonical_blocks_df)} entries.\")\n",
    "\n",
    "\n",
    "# Enrich attestations DataFrame (using Polars throughout)\n",
    "print(\"Enriching attestations DataFrame...\")\n",
    "\n",
    "# Ensure correct types in all_attestations_pl before joining\n",
    "# Assuming 'block_slot_start_date_time' exists from the initial load or previous steps\n",
    "# If 'block_slot_start_date_time' is not already datetime, cast it here. Example:\n",
    "# all_attestations_pl = all_attestations_pl.with_columns(\n",
    "#     pl.col('block_slot_start_date_time').str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S%.f\").dt.replace_time_zone(\"UTC\")\n",
    "# )\n",
    "\n",
    "# Ensure types for join keys match\n",
    "all_attestations_pl = all_attestations_pl.with_columns(\n",
    "    pl.col('validator').cast(pl.UInt32),\n",
    "    pl.col('slot').cast(pl.UInt64), # Match canonical_slot type\n",
    "    pl.col('block_slot').cast(pl.UInt64) # Match beacon_blocks_pl slot type\n",
    ")\n",
    "\n",
    "# 1. Add attester entity\n",
    "enriched_attestations = all_attestations_pl.join(\n",
    "    validators_df.rename({\"entity\": \"attester_entity\"}),\n",
    "    on=\"validator\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Set unknown entities to 'unknown'\n",
    "enriched_attestations = enriched_attestations.with_columns(\n",
    "    pl.col(\"attester_entity\").fill_null(\"unknown\")\n",
    ")\n",
    "\n",
    "# 2. Add canonical block root for the attested slot\n",
    "enriched_attestations = enriched_attestations.join(\n",
    "    canonical_blocks_df,\n",
    "    left_on=\"slot\",\n",
    "    right_on=\"canonical_slot\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# 3. Add proposer info for the block the attestation was included in\n",
    "# Also add block_slot_start_date_time from beacon_blocks_pl\n",
    "enriched_attestations = enriched_attestations.join(\n",
    "    beacon_blocks_pl.select(\n",
    "        \"slot\", \"proposer_index\", \"proposer_entity\", \"slot_start_date_time\"\n",
    "    ).rename({\n",
    "        \"slot\": \"block_slot\", # Join on the block slot containing the attestation\n",
    "        \"proposer_index\": \"block_proposer_index\",\n",
    "        \"proposer_entity\": \"block_proposer_entity\",\n",
    "        \"slot_start_date_time\": \"block_slot_start_date_time_canonical\" # Get canonical time for the block\n",
    "    }),\n",
    "    on=\"block_slot\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Set unknown block proposer entities to 'unknown'\n",
    "enriched_attestations = enriched_attestations.with_columns(\n",
    "    pl.col(\"block_proposer_entity\").fill_null(\"unknown\")\n",
    ")\n",
    "\n",
    "# 4. Calculate head_correct, head_timely, and inclusion_distance\n",
    "enriched_attestations = enriched_attestations.with_columns(\n",
    "    (pl.col('beacon_block_root') == pl.col('canonical_block_root')).alias('head_correct'),\n",
    "    (pl.col('block_slot') - pl.col('slot')).alias('inclusion_distance')\n",
    ").with_columns(\n",
    "    (pl.col('head_correct') & (pl.col('inclusion_distance') == 1)).alias('head_timely')\n",
    ")\n",
    "\n",
    "# 5. Add period column (before/after event) using the canonical block time\n",
    "# Ensure event_date_naive is a naive datetime object compatible with Polars comparison\n",
    "enriched_attestations = enriched_attestations.with_columns(\n",
    "    pl.when(pl.col('block_slot_start_date_time_canonical').dt.replace_time_zone(None) < event_date_naive)\n",
    "    .then(pl.lit(\"before\"))\n",
    "    .otherwise(pl.lit(\"after\"))\n",
    "    .alias(\"period\")\n",
    ")\n",
    "\n",
    "# Finally sort them by block_slot_start_date_time_canonical\n",
    "enriched_attestations = enriched_attestations.sort(\"block_slot_start_date_time\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Attestations DataFrame enriched.\")\n",
    "print(enriched_attestations.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Calculate Aggregated Metrics ---\n",
    "\n",
    "# # 1. Slot Metrics (Performance per slot)\n",
    "# print(\"Calculating slot metrics...\")\n",
    "# slot_metrics_df = enriched_attestations.group_by(\"block_slot\").agg(\n",
    "#     pl.len().alias(\"total_attestations\"),\n",
    "#     pl.sum(\"head_correct\").alias(\"correct_head_count\"),\n",
    "#     pl.first(\"block_slot_start_date_time\"),\n",
    "#     # Keep proposer info associated with the block_slot\n",
    "#     pl.first(\"block_proposer_index\").alias(\"proposer_index\"),\n",
    "#     pl.first(\"block_proposer_entity\").alias(\"proposer_entity\")\n",
    "# ).with_columns(\n",
    "#     (pl.col(\"correct_head_count\") / pl.col(\"total_attestations\") * 100)\n",
    "#     .fill_null(0.0) # Handle division by zero if total_attestations is 0\n",
    "#     .alias(\"correct_head_percentage\")\n",
    "# ).sort(\"block_slot\")\n",
    "\n",
    "# print(\"Slot metrics calculated.\")\n",
    "# print(slot_metrics_df.head())\n",
    "\n",
    "\n",
    "# # 2. Attester Entity Performance (Performance per attesting entity)\n",
    "# print(\"Calculating attester entity performance...\")\n",
    "# attester_entity_performance_df = enriched_attestations.group_by(\"attester_entity\").agg(\n",
    "#     pl.len().alias(\"total_attestations\"),\n",
    "#     pl.sum(\"head_correct\").alias(\"correct_head_count\"),\n",
    "#     pl.mean(\"head_correct\").alias(\"avg_correct_head_percentage_calc\") * 100, # Calculate mean directly\n",
    "#     pl.sum(\"head_timely\").alias(\"timely_head_count\"),\n",
    "#     pl.mean(\"head_timely\").alias(\"avg_timely_head_percentage_calc\") * 100, # Calculate mean directly\n",
    "#     pl.min(\"block_slot_start_date_time\").alias(\"first_attestation_time\"), # Use min/max for time range\n",
    "#     pl.max(\"block_slot_start_date_time\").alias(\"last_attestation_time\")\n",
    "# ).rename({ # Rename calculated columns for clarity\n",
    "#     \"avg_correct_head_percentage_calc\": \"avg_correct_head_percentage\",\n",
    "#     \"avg_timely_head_percentage_calc\": \"avg_timely_head_percentage\"\n",
    "# }).sort(\"avg_correct_head_percentage\", descending=True)\n",
    "\n",
    "\n",
    "# print(\"Attester entity performance calculated.\")\n",
    "# print(attester_entity_performance_df.head())\n",
    "\n",
    "\n",
    "# # 3. Proposer Entity Performance (Accuracy of attestations in blocks proposed by entity)\n",
    "# print(\"Calculating proposer entity performance metrics...\")\n",
    "\n",
    "# # Handle event_date properly to avoid timezone issues\n",
    "# try:\n",
    "#     # Convert to naive datetime if it's timezone-aware\n",
    "#     if isinstance(event_date, pd.Timestamp) and event_date.tzinfo is not None:\n",
    "#         event_date_naive = event_date.tz_localize(None)\n",
    "#     else:\n",
    "#         event_date_naive = pd.to_datetime(event_date)\n",
    "    \n",
    "#     # Create period column based on comparison with naive datetime\n",
    "#     slot_metrics_with_period = slot_metrics_df.with_columns(\n",
    "#         pl.when(pl.col(\"block_slot_start_date_time\") < pl.lit(event_date_naive))\n",
    "#         .then(pl.lit(\"Before\"))\n",
    "#         .otherwise(pl.lit(\"After\"))\n",
    "#         .alias(\"period\")\n",
    "#     )\n",
    "#     print(f\"Period column created using event date: {event_date_naive}\")\n",
    "# except Exception as e:\n",
    "#     # If event_date processing fails, create period column with a default value\n",
    "#     slot_metrics_with_period = slot_metrics_df.with_columns(pl.lit(\"After\").alias(\"period\"))\n",
    "#     print(f\"Using default 'After' period due to event_date processing issue: {e}\")\n",
    "\n",
    "\n",
    "# # Group by proposer entity and period\n",
    "# proposer_entity_performance = slot_metrics_with_period.group_by([\"proposer_entity\", \"period\"]).agg(\n",
    "#     pl.len().alias(\"total_blocks_proposed\"), # Count slots = count blocks proposed\n",
    "#     pl.mean(\"correct_head_percentage\").alias(\"avg_head_vote_accuracy\"),\n",
    "#     pl.min(\"correct_head_percentage\").alias(\"min_head_vote_accuracy\"),\n",
    "#     pl.max(\"correct_head_percentage\").alias(\"max_head_vote_accuracy\"),\n",
    "#     pl.std(\"correct_head_percentage\").alias(\"std_head_vote_accuracy\")\n",
    "# )\n",
    "\n",
    "# # Add timely head metrics (aggregated by attester entity) - replicating original logic's merge\n",
    "# # Note: This merges average *attester* timeliness onto the *proposer* performance summary.\n",
    "# # Consider if a different aggregation (e.g., timeliness of attestations *within* proposed blocks) is desired.\n",
    "# timely_entity_performance_agg = attester_entity_performance_df.select(\n",
    "#     pl.col(\"attester_entity\").alias(\"proposer_entity\"), # Rename to match join key\n",
    "#     \"timely_head_count\",\n",
    "#     \"avg_timely_head_percentage\"\n",
    "# )\n",
    "\n",
    "# proposer_entity_performance = proposer_entity_performance.join(\n",
    "#     timely_entity_performance_agg,\n",
    "#     on=\"proposer_entity\",\n",
    "#     how=\"left\"\n",
    "# )\n",
    "\n",
    "\n",
    "# # Sort by period and average head vote accuracy\n",
    "# proposer_entity_performance = proposer_entity_performance.sort(\n",
    "#     [\"period\", \"avg_head_vote_accuracy\"], descending=[False, True]\n",
    "# )\n",
    "\n",
    "# print(\"Proposer entity performance calculated.\")\n",
    "# print(\"Head vote accuracy and timely head metrics for blocks proposed by each entity (before and after event):\")\n",
    "# print(proposer_entity_performance)\n",
    "\n",
    "# # Display the main dataframes (optional, depends on notebook context)\n",
    "# # print(\"\\nEnriched Attestations Head:\")\n",
    "# # print(enriched_attestations.head())\n",
    "# # print(\"\\nSlot Metrics Head:\")\n",
    "# # print(slot_metrics_df.head())\n",
    "# # print(\"\\nAttester Entity Performance Head:\")\n",
    "# # print(attester_entity_performance_df.head())\n",
    "# # print(\"\\nProposer Entity Performance:\")\n",
    "# # print(proposer_entity_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualization for head timely percentage by attester entity, split by period\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Calculate head_timely percentage per attester_entity and period\n",
    "attester_performance = enriched_attestations.group_by([\"attester_entity\", \"period\"]).agg(\n",
    "    pl.sum(\"head_timely\").alias(\"timely_head_count\"),\n",
    "    pl.count(\"head_timely\").alias(\"total_attestations\")\n",
    ").with_columns(\n",
    "    (pl.col(\"timely_head_count\") / pl.col(\"total_attestations\") * 100).alias(\"head_timely_percentage\")\n",
    ")\n",
    "\n",
    "# Calculate overall average per period\n",
    "overall_avg = attester_performance.group_by(\"period\").agg(\n",
    "    pl.mean(\"head_timely_percentage\").alias(\"overall_average\")\n",
    ")\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "attester_data = attester_performance.to_pandas()\n",
    "overall_avg_data = overall_avg.to_pandas()\n",
    "\n",
    "# Get top 25 entities by attestation count\n",
    "top_entities = attester_performance.group_by(\"attester_entity\").agg(\n",
    "    pl.sum(\"total_attestations\").alias(\"attestation_count\")\n",
    ").sort(\"attestation_count\", descending=True)[\"attester_entity\"].to_list()\n",
    "\n",
    "# Filter data for top entities\n",
    "top_entities_data = attester_data[attester_data[\"attester_entity\"].isin(top_entities)]\n",
    "\n",
    "# Create a summary table with before/after values\n",
    "summary_table = top_entities_data.pivot_table(\n",
    "    index='attester_entity',\n",
    "    columns='period',\n",
    "    values='head_timely_percentage',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "# Add network average to the summary table\n",
    "network_avg_row = pd.DataFrame({\n",
    "    'attester_entity': ['network average'],\n",
    "    'before': [overall_avg_data[overall_avg_data['period'] == 'before']['overall_average'].values[0]],\n",
    "    'after': [overall_avg_data[overall_avg_data['period'] == 'after']['overall_average'].values[0]]\n",
    "})\n",
    "summary_table = pd.concat([summary_table, network_avg_row])\n",
    "\n",
    "# Create interactive subplot figure\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Before\", \"After\"),\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing=0.05 # Add a bit more space between plots\n",
    ")\n",
    "\n",
    "# Colors for entities\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "# For each period (before/after)\n",
    "for i, period in enumerate([\"before\", \"after\"]):\n",
    "    col = i + 1  # Plotly is 1-indexed\n",
    "\n",
    "    # Filter data for this period\n",
    "    period_data = top_entities_data[top_entities_data[\"period\"] == period]\n",
    "\n",
    "    # Add scatter plot for each entity\n",
    "    for j, entity in enumerate(top_entities):\n",
    "        entity_data = period_data[period_data[\"attester_entity\"] == entity]\n",
    "        if not entity_data.empty:\n",
    "            # Get before/after values for legend name\n",
    "            before_val = summary_table[summary_table['attester_entity'] == entity]['before'].values[0] if 'before' in summary_table.columns else np.nan\n",
    "            after_val = summary_table[summary_table['attester_entity'] == entity]['after'].values[0] if 'after' in summary_table.columns else np.nan\n",
    "\n",
    "            # Format legend name with values\n",
    "            legend_name = f\"{entity} (Before: {before_val:.2f}%, After: {after_val:.2f}%)\"\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[entity] * len(entity_data),\n",
    "                    y=entity_data[\"head_timely_percentage\"],\n",
    "                    mode='markers',\n",
    "                    name=legend_name,\n",
    "                    marker=dict(\n",
    "                        size=10, # Slightly smaller markers\n",
    "                        color=colors[j % len(colors)],\n",
    "                        opacity=0.7\n",
    "                    ),\n",
    "                    hovertemplate=\n",
    "                    '<b>%{text}</b><br>' +\n",
    "                    'Head Timely: %{y:.2f}%<br>' +\n",
    "                    '<extra></extra>',\n",
    "                    text=[entity] * len(entity_data),\n",
    "                    showlegend=(col == 2)  # Only show legend for the second plot\n",
    "                ),\n",
    "                row=1, col=col\n",
    "            )\n",
    "\n",
    "    # Add a line for the overall average\n",
    "    overall_avg_value = overall_avg_data[overall_avg_data[\"period\"] == period][\"overall_average\"].values[0]\n",
    "\n",
    "    # Get both before/after values for network average legend\n",
    "    before_avg = overall_avg_data[overall_avg_data[\"period\"] == \"before\"][\"overall_average\"].values[0]\n",
    "    after_avg = overall_avg_data[overall_avg_data[\"period\"] == \"after\"][\"overall_average\"].values[0]\n",
    "    network_legend = f\"Network Average (Before: {before_avg:.2f}%, After: {after_avg:.2f}%)\"\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[entity for entity in top_entities],\n",
    "            y=[overall_avg_value] * len(top_entities),\n",
    "            mode='lines',\n",
    "            line=dict(color='red', width=2, dash='dash'),\n",
    "            name=network_legend,\n",
    "            showlegend=(col == 2)  # Only show legend for the second plot\n",
    "        ),\n",
    "        row=1, col=col\n",
    "    )\n",
    "# Update layout for main figure, show the title in the middle\n",
    "fig.update_layout(\n",
    "    # autosize=True, # Disable autosize to set manual dimensions\n",
    "    width=1400, # Increase width\n",
    "    height=700, # Increase height\n",
    "    hovermode=\"closest\",\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=-0.5, # Move legend further down\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        orientation=\"h\"\n",
    "    ),\n",
    "    title=dict(\n",
    "        text=\"Hoodi Head Timely Percentage by Attester Entity<br><sup>Shows the percentage of correct head attestations that were immediately included in the next block<br>Grouped by attester entity, split by period (before/after event Electra fork)</sup>\",\n",
    "        x=0.5,  # Center the title\n",
    "        xanchor=\"center\",\n",
    "        y=0.95,  # Move title down a bit to create more padding\n",
    "        pad=dict(t=20)  # Add padding between title and plot\n",
    "    ),\n",
    "    margin=dict(l=50, r=50, t=120, b=150) # Increased top margin for more space between title and plots\n",
    ")\n",
    "\n",
    "# Update axes - always show y ticks\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Head Timely Percentage (%)\",\n",
    "    range=[0, 100],\n",
    "    gridcolor='lightgray',\n",
    "    showticklabels=True,\n",
    "    tickmode='linear',\n",
    "    tick0=0,\n",
    "    dtick=5\n",
    ")\n",
    "# Rotate x-axis labels to prevent overlap\n",
    "fig.update_xaxes(\n",
    "    showticklabels=True,\n",
    "    tickangle=90 # Rotate labels\n",
    ")\n",
    "\n",
    "# Add logo to the top right corner\n",
    "ethpandaops_path = \"../../assets/content/ethpandaops.png\"\n",
    "fig.add_layout_image(\n",
    "    dict(\n",
    "        source=ethpandaops_path,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.05, y=1.1,\n",
    "        sizex=0.35, sizey=0.35,\n",
    "        xanchor=\"right\", yanchor=\"bottom\",\n",
    "        opacity=1,\n",
    "        layer=\"above\"\n",
    "    )\n",
    ")\n",
    "\n",
    "xatu_path = \"../../assets/content/xatu.png\"\n",
    "fig.add_layout_image(\n",
    "    dict(\n",
    "        source=xatu_path,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=1.0, y=1.1,\n",
    "        sizex=0.35, sizey=0.35,\n",
    "        xanchor=\"right\", yanchor=\"bottom\",\n",
    "        opacity=1,\n",
    "        layer=\"above\"\n",
    "    )\n",
    ")   \n",
    "\n",
    "# Display the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualization for head timely percentage by proposer entity, split by period\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Calculate head_timely percentage per proposer_entity and period\n",
    "proposer_performance = enriched_attestations.group_by([\"block_proposer_entity\", \"period\"]).agg(\n",
    "    pl.sum(\"head_timely\").alias(\"timely_head_count\"),\n",
    "    pl.count(\"head_timely\").alias(\"total_attestations\")\n",
    ").with_columns(\n",
    "    (pl.col(\"timely_head_count\") / pl.col(\"total_attestations\") * 100).alias(\"head_timely_percentage\")\n",
    ")\n",
    "\n",
    "# Calculate overall average per period\n",
    "overall_avg = proposer_performance.group_by(\"period\").agg(\n",
    "    pl.mean(\"head_timely_percentage\").alias(\"overall_average\")\n",
    ")\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "proposer_data = proposer_performance.to_pandas()\n",
    "overall_avg_data = overall_avg.to_pandas()\n",
    "\n",
    "# Get top 25 entities by attestation count\n",
    "top_entities = proposer_performance.group_by(\"block_proposer_entity\").agg(\n",
    "    pl.sum(\"total_attestations\").alias(\"attestation_count\")\n",
    ").sort(\"attestation_count\", descending=True)[\"block_proposer_entity\"].to_list()\n",
    "\n",
    "# Filter data for top entities\n",
    "top_entities_data = proposer_data[proposer_data[\"block_proposer_entity\"].isin(top_entities)]\n",
    "\n",
    "# Create a summary table with before/after values\n",
    "summary_table = top_entities_data.pivot_table(\n",
    "    index='block_proposer_entity',\n",
    "    columns='period',\n",
    "    values='head_timely_percentage',\n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "# Add network average to the summary table\n",
    "network_avg_row = pd.DataFrame({\n",
    "    'block_proposer_entity': ['network average'],\n",
    "    'before': [overall_avg_data[overall_avg_data['period'] == 'before']['overall_average'].values[0]],\n",
    "    'after': [overall_avg_data[overall_avg_data['period'] == 'after']['overall_average'].values[0]]\n",
    "})\n",
    "summary_table = pd.concat([summary_table, network_avg_row])\n",
    "\n",
    "# Create interactive subplot figure\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Before\", \"After\"),\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing=0.05 # Add a bit more space between plots\n",
    ")\n",
    "\n",
    "# Colors for entities\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "# For each period (before/after)\n",
    "for i, period in enumerate([\"before\", \"after\"]):\n",
    "    col = i + 1  # Plotly is 1-indexed\n",
    "\n",
    "    # Filter data for this period\n",
    "    period_data = top_entities_data[top_entities_data[\"period\"] == period]\n",
    "\n",
    "    # Add scatter plot for each entity\n",
    "    for j, entity in enumerate(top_entities):\n",
    "        entity_data = period_data[period_data[\"block_proposer_entity\"] == entity]\n",
    "        if not entity_data.empty:\n",
    "            # Get before/after values for legend name\n",
    "            before_val = summary_table[summary_table['block_proposer_entity'] == entity]['before'].values[0] if 'before' in summary_table.columns else np.nan\n",
    "            after_val = summary_table[summary_table['block_proposer_entity'] == entity]['after'].values[0] if 'after' in summary_table.columns else np.nan\n",
    "\n",
    "            # Format legend name with values\n",
    "            legend_name = f\"{entity} (Before: {before_val:.2f}%, After: {after_val:.2f}%)\"\n",
    "\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=[entity] * len(entity_data),\n",
    "                    y=entity_data[\"head_timely_percentage\"],\n",
    "                    mode='markers',\n",
    "                    name=legend_name,\n",
    "                    marker=dict(\n",
    "                        size=10, # Slightly smaller markers\n",
    "                        color=colors[j % len(colors)],\n",
    "                        opacity=0.7\n",
    "                    ),\n",
    "                    hovertemplate=\n",
    "                    '<b>%{text}</b><br>' +\n",
    "                    'Head Timely: %{y:.2f}%<br>' +\n",
    "                    '<extra></extra>',\n",
    "                    text=[entity] * len(entity_data),\n",
    "                    showlegend=(col == 2)  # Only show legend for the second plot\n",
    "                ),\n",
    "                row=1, col=col\n",
    "            )\n",
    "\n",
    "    # Add a line for the overall average\n",
    "    overall_avg_value = overall_avg_data[overall_avg_data[\"period\"] == period][\"overall_average\"].values[0]\n",
    "\n",
    "    # Get both before/after values for network average legend\n",
    "    before_avg = overall_avg_data[overall_avg_data[\"period\"] == \"before\"][\"overall_average\"].values[0]\n",
    "    after_avg = overall_avg_data[overall_avg_data[\"period\"] == \"after\"][\"overall_average\"].values[0]\n",
    "    network_legend = f\"Network Average (Before: {before_avg:.2f}%, After: {after_avg:.2f}%)\"\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[entity for entity in top_entities],\n",
    "            y=[overall_avg_value] * len(top_entities),\n",
    "            mode='lines',\n",
    "            line=dict(color='red', width=2, dash='dash'),\n",
    "            name=network_legend,\n",
    "            showlegend=(col == 2)  # Only show legend for the second plot\n",
    "        ),\n",
    "        row=1, col=col\n",
    "    )\n",
    "# Update layout for main figure, show the title in the middle\n",
    "fig.update_layout(\n",
    "    # autosize=True, # Disable autosize to set manual dimensions\n",
    "    width=1400, # Increase width\n",
    "    height=700, # Increase height\n",
    "    hovermode=\"closest\",\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=-0.5, # Move legend further down\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        orientation=\"h\"\n",
    "    ),\n",
    "    title=dict(\n",
    "        text=\"Hoodi Head Timely Percentage by Block Proposer Entity<br><sup>Shows the percentage of correct head attestations that were immediately included in the block by the proposer<br>Grouped by proposer entity, split by period (before/after event Electra fork)</sup>\",\n",
    "        x=0.5,  # Center the title\n",
    "        xanchor=\"center\",\n",
    "        y=0.95,  # Move title down a bit to create more padding\n",
    "        pad=dict(t=20)  # Add padding between title and plot\n",
    "    ),\n",
    "    margin=dict(l=50, r=50, t=120, b=150) # Increased top margin for more space between title and plots\n",
    ")\n",
    "\n",
    "# Update axes - always show y ticks\n",
    "fig.update_yaxes(\n",
    "    title_text=\"Head Timely Percentage (%)\",\n",
    "    range=[0, 100],\n",
    "    gridcolor='lightgray',\n",
    "    showticklabels=True,\n",
    "    tickmode='linear',\n",
    "    tick0=0,\n",
    "    dtick=5\n",
    ")\n",
    "# Rotate x-axis labels to prevent overlap\n",
    "fig.update_xaxes(\n",
    "    showticklabels=True,\n",
    "    tickangle=90 # Rotate labels\n",
    ")\n",
    "\n",
    "# Add logo to the top right corner\n",
    "ethpandaops_path = \"../../assets/content/ethpandaops.png\"\n",
    "fig.add_layout_image(\n",
    "    dict(\n",
    "        source=ethpandaops_path,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.05, y=1.1,\n",
    "        sizex=0.35, sizey=0.35,\n",
    "        xanchor=\"right\", yanchor=\"bottom\",\n",
    "        opacity=1,\n",
    "        layer=\"above\"\n",
    "    )\n",
    ")\n",
    "\n",
    "xatu_path = \"../../assets/content/xatu.png\"\n",
    "fig.add_layout_image(\n",
    "    dict(\n",
    "        source=xatu_path,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=1.0, y=1.1,\n",
    "        sizex=0.25, sizey=0.25,\n",
    "        xanchor=\"right\", yanchor=\"bottom\",\n",
    "        opacity=1,\n",
    "        layer=\"above\"\n",
    "    )\n",
    ")   \n",
    "\n",
    "\n",
    "# Display the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot showing average head timely percentage per attester entity per day\n",
    "\n",
    "# Extract date from datetime\n",
    "enriched_attestations = enriched_attestations.with_columns(\n",
    "    pl.col(\"block_slot_start_date_time\").dt.date().alias(\"date\")\n",
    ")\n",
    "\n",
    "# Calculate head_timely percentage per attester_entity and date\n",
    "daily_performance = enriched_attestations.group_by([\"attester_entity\", \"date\"]).agg(\n",
    "    pl.sum(\"head_timely\").alias(\"timely_head_count\"),\n",
    "    pl.len().alias(\"total_attestations\")\n",
    ").with_columns(\n",
    "    (pl.col(\"timely_head_count\") / pl.col(\"total_attestations\") * 100).alias(\"head_timely_percentage\")\n",
    ")\n",
    "\n",
    "# Calculate overall average per date\n",
    "daily_overall_avg = daily_performance.group_by(\"date\").agg(\n",
    "    pl.mean(\"head_timely_percentage\").alias(\"overall_average\")\n",
    ")\n",
    "\n",
    "# Get entity sizes from validators map\n",
    "entity_counts = {}\n",
    "for validator_index, entity in validators.items():\n",
    "    if entity in entity_counts:\n",
    "        entity_counts[entity] += 1\n",
    "    else:\n",
    "        entity_counts[entity] = 1\n",
    "\n",
    "# Sort entities by size and get top 15\n",
    "top_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "top_15_entities = [entity for entity, count in top_entities]\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "daily_data = daily_performance.to_pandas()\n",
    "daily_avg_data = daily_overall_avg.to_pandas()\n",
    "\n",
    "# Sort data by date to ensure lines are drawn correctly\n",
    "daily_avg_data = daily_avg_data.sort_values(\"date\")\n",
    "\n",
    "# Create a subplot figure with one subplot per entity\n",
    "fig_daily = make_subplots(\n",
    "    rows=5, \n",
    "    cols=3,\n",
    "    subplot_titles=[entity for entity in top_15_entities],\n",
    "    vertical_spacing=0.1,\n",
    "    horizontal_spacing=0.05\n",
    ")\n",
    "\n",
    "# Add a line for each top entity in its own subplot\n",
    "for i, entity in enumerate(top_15_entities):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    \n",
    "    entity_data = daily_data[daily_data[\"attester_entity\"] == entity]\n",
    "    if not entity_data.empty:\n",
    "        # Sort by date to ensure proper line drawing\n",
    "        entity_data = entity_data.sort_values(\"date\")\n",
    "        \n",
    "        # Add entity line\n",
    "        fig_daily.add_trace(\n",
    "            go.Scatter(\n",
    "                x=entity_data[\"date\"],\n",
    "                y=entity_data[\"head_timely_percentage\"],\n",
    "                mode='lines',\n",
    "                name=entity,\n",
    "                line=dict(color=colors[i % len(colors)]),\n",
    "                showlegend=False,\n",
    "                hovertemplate=\n",
    "                '<b>%{text}</b><br>' +\n",
    "                'Date: %{x}<br>' +\n",
    "                'Head Timely: %{y:.2f}%<br>' +\n",
    "                '<extra></extra>',\n",
    "                text=[entity] * len(entity_data)\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "        \n",
    "        # Add network average line to the same subplot\n",
    "        fig_daily.add_trace(\n",
    "            go.Scatter(\n",
    "                x=daily_avg_data[\"date\"],\n",
    "                y=daily_avg_data[\"overall_average\"],\n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=2, dash='dash'),\n",
    "                name='Network Average',\n",
    "                showlegend=(i == 0),  # Only show in legend once\n",
    "                hovertemplate=\n",
    "                '<b>Network Average</b><br>' +\n",
    "                'Date: %{x}<br>' +\n",
    "                'Head Timely: %{y:.2f}%<br>' +\n",
    "                '<extra></extra>',\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "        \n",
    "        # Convert event_date to timestamp for plotly\n",
    "        event_date_timestamp = event_date.timestamp() * 1000  # Convert to milliseconds for plotly\n",
    "        \n",
    "        # Add vertical line for Electra fork event using timestamp\n",
    "        fig_daily.add_vline(\n",
    "            x=event_date_timestamp,\n",
    "            line=dict(color=\"green\", width=2, dash=\"dash\"),\n",
    "            annotation_text=\"Electra Fork\",\n",
    "            annotation_position=\"top right\",\n",
    "            row=row, col=col\n",
    "        )\n",
    "        \n",
    "        # Set y-axis range for each subplot\n",
    "        fig_daily.update_yaxes(range=[0, 100], row=row, col=col)\n",
    "\n",
    "# Update layout\n",
    "fig_daily.update_layout(\n",
    "    width=1600,\n",
    "    height=1200,\n",
    "    title=dict(\n",
    "        text=\"Daily Head Timely Percentage by Top Attester Entities vs Network Average<br><sup>Shows the percentage of correct head attestations that were immediately included in the next block over time</sup>\",\n",
    "        x=0.5,\n",
    "        xanchor=\"center\",\n",
    "        y=0.98\n",
    "    ),\n",
    "    hovermode=\"closest\",\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=-0.05,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        orientation=\"h\"\n",
    "    ),\n",
    "    margin=dict(l=50, r=50, t=120, b=100)\n",
    ")\n",
    "\n",
    "# Update all x-axes\n",
    "fig_daily.update_xaxes(\n",
    "    tickangle=45,\n",
    "    gridcolor='lightgray'\n",
    ")\n",
    "\n",
    "# Update all y-axes\n",
    "fig_daily.update_yaxes(\n",
    "    title_text=\"Head Timely %\",\n",
    "    gridcolor='lightgray',\n",
    "    tickmode='linear',\n",
    "    tick0=0,\n",
    "    dtick=20\n",
    ")\n",
    "\n",
    "# Add logos\n",
    "fig_daily.add_layout_image(\n",
    "    dict(\n",
    "        source=ethpandaops_path,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.05, y=1.05,\n",
    "        sizex=0.06, sizey=0.06,\n",
    "        xanchor=\"right\", yanchor=\"bottom\",\n",
    "        opacity=1,\n",
    "        layer=\"above\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_daily.add_layout_image(\n",
    "    dict(\n",
    "        source=xatu_path,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=1.0, y=1.05,\n",
    "        sizex=0.06, sizey=0.06,\n",
    "        xanchor=\"right\", yanchor=\"bottom\",\n",
    "        opacity=1,\n",
    "        layer=\"above\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the interactive plot\n",
    "fig_daily.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot showing average head timely percentage per attester entity per day\n",
    "\n",
    "# Extract date from datetime\n",
    "enriched_attestations = enriched_attestations.with_columns(\n",
    "    pl.col(\"block_slot_start_date_time\").dt.date().alias(\"date\")\n",
    ")\n",
    "\n",
    "# Calculate head_timely percentage per block_proposer_entity and date\n",
    "daily_performance = enriched_attestations.group_by([\"block_proposer_entity\", \"date\"]).agg(\n",
    "    pl.sum(\"head_timely\").alias(\"timely_head_count\"),\n",
    "    pl.len().alias(\"total_attestations\")\n",
    ").with_columns(\n",
    "    (pl.col(\"timely_head_count\") / pl.col(\"total_attestations\") * 100).alias(\"head_timely_percentage\")\n",
    ")\n",
    "\n",
    "# Calculate overall average per date\n",
    "daily_overall_avg = daily_performance.group_by(\"date\").agg(\n",
    "    pl.mean(\"head_timely_percentage\").alias(\"overall_average\")\n",
    ")\n",
    "\n",
    "# Get the top 15 entities by total attestation count\n",
    "entity_attestation_counts = enriched_attestations.group_by(\"block_proposer_entity\").agg(\n",
    "    pl.len().alias(\"total_attestations\")\n",
    ").sort(\"total_attestations\", descending=True).limit(15)\n",
    "\n",
    "top_15_entities = entity_attestation_counts[\"block_proposer_entity\"].to_list()\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "daily_data = daily_performance.to_pandas()\n",
    "daily_avg_data = daily_overall_avg.to_pandas()\n",
    "\n",
    "# Sort data by date to ensure lines are drawn correctly\n",
    "daily_avg_data = daily_avg_data.sort_values(\"date\")\n",
    "\n",
    "# Create a subplot figure with one subplot per entity\n",
    "fig_daily = make_subplots(\n",
    "    rows=5, \n",
    "    cols=3,\n",
    "    subplot_titles=[entity for entity in top_15_entities],\n",
    "    vertical_spacing=0.1,\n",
    "    horizontal_spacing=0.05\n",
    ")\n",
    "\n",
    "# Add a line for each top entity in its own subplot\n",
    "for i, entity in enumerate(top_15_entities):\n",
    "    row = (i // 3) + 1\n",
    "    col = (i % 3) + 1\n",
    "    \n",
    "    entity_data = daily_data[daily_data[\"block_proposer_entity\"] == entity]\n",
    "    if not entity_data.empty:\n",
    "        # Sort by date to ensure proper line drawing\n",
    "        entity_data = entity_data.sort_values(\"date\")\n",
    "        \n",
    "        # Add entity line\n",
    "        fig_daily.add_trace(\n",
    "            go.Scatter(\n",
    "                x=entity_data[\"date\"],\n",
    "                y=entity_data[\"head_timely_percentage\"],\n",
    "                mode='lines',\n",
    "                name=entity,\n",
    "                line=dict(color=colors[i % len(colors)]),\n",
    "                showlegend=False,\n",
    "                hovertemplate=\n",
    "                '<b>%{text}</b><br>' +\n",
    "                'Date: %{x}<br>' +\n",
    "                'Head Timely: %{y:.2f}%<br>' +\n",
    "                '<extra></extra>',\n",
    "                text=[entity] * len(entity_data)\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "        \n",
    "        # Add network average line to the same subplot\n",
    "        fig_daily.add_trace(\n",
    "            go.Scatter(\n",
    "                x=daily_avg_data[\"date\"],\n",
    "                y=daily_avg_data[\"overall_average\"],\n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=2, dash='dash'),\n",
    "                name='Network Average',\n",
    "                showlegend=(i == 0),  # Only show in legend once\n",
    "                hovertemplate=\n",
    "                '<b>Network Average</b><br>' +\n",
    "                'Date: %{x}<br>' +\n",
    "                'Head Timely: %{y:.2f}%<br>' +\n",
    "                '<extra></extra>',\n",
    "            ),\n",
    "            row=row, col=col\n",
    "        )\n",
    "        \n",
    "        # Convert event_date to timestamp for plotly\n",
    "        event_date_timestamp = event_date.timestamp() * 1000  # Convert to milliseconds for plotly\n",
    "        \n",
    "        # Add vertical line for Electra fork event using timestamp\n",
    "        fig_daily.add_vline(\n",
    "            x=event_date_timestamp,\n",
    "            line=dict(color=\"green\", width=2, dash=\"dash\"),\n",
    "            annotation_text=\"Electra Fork\",\n",
    "            annotation_position=\"top right\",\n",
    "            row=row, col=col\n",
    "        )\n",
    "        \n",
    "        # Set y-axis range for each subplot\n",
    "        fig_daily.update_yaxes(range=[0, 100], row=row, col=col)\n",
    "\n",
    "# Update layout\n",
    "fig_daily.update_layout(\n",
    "    width=1600,\n",
    "    height=1200,\n",
    "    title=dict(\n",
    "        text=\"Daily Head Timely Percentage by Top Block Proposers Entities vs Network Average<br><sup>Shows the percentage of correct head attestations that the block proposer immediately included in the next block over time</sup>\",\n",
    "        x=0.5,\n",
    "        xanchor=\"center\",\n",
    "        y=0.98\n",
    "    ),\n",
    "    hovermode=\"closest\",\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=-0.05,\n",
    "        xanchor=\"center\",\n",
    "        x=0.5,\n",
    "        orientation=\"h\"\n",
    "    ),\n",
    "    margin=dict(l=50, r=50, t=120, b=100)\n",
    ")\n",
    "\n",
    "# Update all x-axes\n",
    "fig_daily.update_xaxes(\n",
    "    tickangle=45,\n",
    "    gridcolor='lightgray'\n",
    ")\n",
    "\n",
    "# Update all y-axes\n",
    "fig_daily.update_yaxes(\n",
    "    title_text=\"Head Timely %\",\n",
    "    gridcolor='lightgray',\n",
    "    tickmode='linear',\n",
    "    tick0=0,\n",
    "    dtick=20\n",
    ")\n",
    "\n",
    "# Add logos\n",
    "fig_daily.add_layout_image(\n",
    "    dict(\n",
    "        source=ethpandaops_path,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.05, y=1.05,\n",
    "        sizex=0.06, sizey=0.06,\n",
    "        xanchor=\"right\", yanchor=\"bottom\",\n",
    "        opacity=1,\n",
    "        layer=\"above\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig_daily.add_layout_image(\n",
    "    dict(\n",
    "        source=xatu_path,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=1.0, y=1.05,\n",
    "        sizex=0.06, sizey=0.06,\n",
    "        xanchor=\"right\", yanchor=\"bottom\",\n",
    "        opacity=1,\n",
    "        layer=\"above\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display the interactive plot\n",
    "fig_daily.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

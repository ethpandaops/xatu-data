{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate a potential blob bump on mainnet\n",
    "See .env.example to set the environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!pip install jupysql clickhouse_sqlalchemy matplotlib python-dotenv pandas seaborn imageio > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Load and display config\n",
    "start_date = os.getenv('START_DATE_TIME')\n",
    "if not start_date:\n",
    "    raise ValueError(\"START_DATE_TIME environment variable is required\")\n",
    "\n",
    "end_date = os.getenv('END_DATE_TIME')\n",
    "if not end_date:\n",
    "    end_date = datetime.now().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "network = os.getenv('NETWORK')\n",
    "\n",
    "print(f\"start_date: {start_date}\")\n",
    "print(f\"end_date: {end_date}\")\n",
    "\n",
    "# Convert start and end dates to datetime objects\n",
    "try:\n",
    "    start_dt = datetime.fromisoformat(start_date.replace(\"Z\", \"+00:00\"))\n",
    "except ValueError as e:\n",
    "    raise ValueError(f\"Invalid start date format. Date must be in ISO format. Error: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    if isinstance(end_date, datetime):\n",
    "        end_dt = end_date\n",
    "    else:\n",
    "        end_dt = datetime.fromisoformat(str(end_date).replace(\"Z\", \"+00:00\"))\n",
    "except ValueError as e:\n",
    "    raise ValueError(f\"Invalid end date format. Date must be in ISO format. Error: {str(e)}\")\n",
    "\n",
    "# Calculate the difference in hours\n",
    "hours = (end_dt - start_dt).total_seconds() / 3600\n",
    "\n",
    "print(f\"hours: {hours}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to ClickHouse\n",
    "import os\n",
    "username = os.getenv('XATU_CLICKHOUSE_USERNAME')\n",
    "password = os.getenv('XATU_CLICKHOUSE_PASSWORD')\n",
    "host = os.getenv('XATU_CLICKHOUSE_HOST')\n",
    "\n",
    "\n",
    "db_url = f\"clickhouse+http://{username}:{password}@{host}:443/default?protocol=https\"\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for MEV relay delivered slots\n",
    "block_query = text(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        slot\n",
    "    FROM mev_relay_proposer_payload_delivered FINAL\n",
    "    WHERE\n",
    "        slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "        AND meta_network_name = 'mainnet'\n",
    "    ORDER BY slot ASC\n",
    "\"\"\")\n",
    "\n",
    "result = connection.execute(block_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')})\n",
    "mev_slots = pd.DataFrame(result.fetchall(), columns=['slot'])\n",
    "\n",
    "print(mev_slots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query for all slots in time window\n",
    "all_slots_query = text(\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        slot,\n",
    "        slot_start_date_time as time\n",
    "    FROM beacon_api_eth_v1_events_block FINAL \n",
    "    WHERE\n",
    "        slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "        AND meta_network_name = 'mainnet'\n",
    "    ORDER BY slot ASC\n",
    "\"\"\")\n",
    "\n",
    "result = connection.execute(all_slots_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')})\n",
    "all_slots = pd.DataFrame(result.fetchall(), columns=['slot', 'time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get block sizes for all slots\n",
    "print(\"Getting block sizes...\")\n",
    "block_sizes_query = text(\"\"\"\n",
    "    SELECT \n",
    "        slot,\n",
    "        block_total_bytes_compressed,\n",
    "        slot_start_date_time as time,\n",
    "        proposer_index,\n",
    "        block_root\n",
    "    FROM canonical_beacon_block FINAL\n",
    "    WHERE \n",
    "        slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "        AND meta_network_name = 'mainnet'\n",
    "\"\"\")\n",
    "block_sizes = pd.DataFrame(\n",
    "    connection.execute(block_sizes_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')}).fetchall(),\n",
    "    columns=['slot', 'block_total_bytes_compressed', 'time', 'proposer_index', 'block_root']\n",
    ")\n",
    "\n",
    "print(\"Getting proposer entities...\")\n",
    "# Get proposer entities\n",
    "proposer_query = text(\"\"\"\n",
    "    SELECT \n",
    "        `index` as proposer_index,\n",
    "        entity\n",
    "    FROM ethseer_validator_entity\n",
    "    WHERE \n",
    "        meta_network_name = 'mainnet'\n",
    "\"\"\")\n",
    "proposer_entities = pd.DataFrame(\n",
    "    connection.execute(proposer_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')}).fetchall(),\n",
    "    columns=['proposer_index', 'entity']\n",
    ")\n",
    "\n",
    "print(\"Getting blob sidecar data...\")\n",
    "# Get blob sidecar data for our slots\n",
    "blob_sidecars_query = text(\"\"\"\n",
    "    SELECT \n",
    "        slot,\n",
    "        COUNT(*) as num_blobs,\n",
    "        SUM(blob_size) as total_blob_size\n",
    "    FROM canonical_beacon_blob_sidecar FINAL\n",
    "    WHERE\n",
    "        slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "        AND meta_network_name = 'mainnet'\n",
    "    GROUP BY slot, toDate(slot_start_date_time)\n",
    "\"\"\")\n",
    "blob_sidecars = pd.DataFrame(\n",
    "    connection.execute(blob_sidecars_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')}).fetchall(),\n",
    "    columns=['slot', 'num_blobs', 'total_blob_size']\n",
    ")\n",
    "\n",
    "print(\"Getting first seen in p2p data...\")\n",
    "first_seen_in_p2p_query = text(\"\"\"\n",
    "    SELECT\n",
    "        slot,\n",
    "        block as block_root,\n",
    "        MIN(propagation_slot_start_diff) as first_seen_in_p2p\n",
    "    FROM libp2p_gossipsub_beacon_block FINAL\n",
    "    WHERE\n",
    "        slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "        AND meta_network_name = 'mainnet'\n",
    "    GROUP BY slot, block\n",
    "\"\"\")\n",
    "first_seen_in_p2p = pd.DataFrame(\n",
    "    connection.execute(first_seen_in_p2p_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')}).fetchall(),\n",
    "    columns=['slot', 'block_root', 'first_seen_in_p2p']\n",
    ")\n",
    "\n",
    "print(\"Getting first seen attestation for block in p2p data...\")\n",
    "first_seen_attestation_in_p2p_query = text(\"\"\"\n",
    "    SELECT\n",
    "        slot,\n",
    "        beacon_block_root as block_root,\n",
    "        MIN(propagation_slot_start_diff) as first_seen_attestation_in_p2p\n",
    "    FROM libp2p_gossipsub_beacon_attestation FINAL\n",
    "    WHERE\n",
    "        slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "        AND meta_network_name = 'mainnet'\n",
    "    GROUP BY slot, beacon_block_root\n",
    "\"\"\")\n",
    "first_seen_attestation_in_p2p = pd.DataFrame(\n",
    "    connection.execute(first_seen_attestation_in_p2p_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')}).fetchall(),\n",
    "    columns=['slot', 'block_root', 'first_seen_attestation_in_p2p']\n",
    ")\n",
    "\n",
    "print(\"Getting arrival times...\")\n",
    "aggregated_arrival_times_query = text(\"\"\"\n",
    "    WITH arrival_times AS (\n",
    "        SELECT\n",
    "            slot,\n",
    "            slot_start_date_time,\n",
    "            meta_client_name,\n",
    "            propagation_slot_start_diff\n",
    "        FROM beacon_api_eth_v1_events_head FINAL\n",
    "        WHERE\n",
    "            slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "            AND meta_client_name != ''\n",
    "            AND meta_client_name IS NOT NULL\n",
    "            AND meta_network_name = 'mainnet'\n",
    "            AND propagation_slot_start_diff < 6000\n",
    "    )\n",
    "    SELECT\n",
    "        slot,\n",
    "        CASE\n",
    "            WHEN meta_client_name NOT LIKE 'ethpandaops%' THEN 'home users'\n",
    "            ELSE 'ethpandaops'\n",
    "        END as observed_by_group,\n",
    "        min(propagation_slot_start_diff) as min_arrival_time,\n",
    "        quantile(0.50)(propagation_slot_start_diff) as p50_arrival_time,\n",
    "        quantile(0.90)(propagation_slot_start_diff) as p90_arrival_time,\n",
    "        quantile(0.99)(propagation_slot_start_diff) as p99_arrival_time\n",
    "    FROM arrival_times\n",
    "    GROUP BY \n",
    "        slot,\n",
    "        CASE\n",
    "            WHEN meta_client_name NOT LIKE 'ethpandaops%' THEN 'home users'\n",
    "            ELSE 'ethpandaops'\n",
    "        END\n",
    "    ORDER BY slot\n",
    "\"\"\")\n",
    "\n",
    "aggregated_arrival_times = pd.DataFrame(\n",
    "    connection.execute(aggregated_arrival_times_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')}).fetchall(),\n",
    "    columns=['slot', 'observed_by_group', 'min_arrival_time', 'p50_arrival_time', 'p90_arrival_time', 'p99_arrival_time']\n",
    ")\n",
    "\n",
    "# Convert blob_sidecars to a dictionary\n",
    "blob_sidecars_map = blob_sidecars.set_index('slot').to_dict(orient='index')\n",
    "\n",
    "# Build combined data using block_sizes as the canonical list of slots\n",
    "combined_data = block_sizes.copy()\n",
    "combined_data = combined_data.merge(proposer_entities, on='proposer_index', how='left')\n",
    "# combined_data = combined_data.merge(first_seen_in_p2p, on=['slot', 'block_root'], how='left')\n",
    "# combined_data = combined_data.merge(first_seen_attestation_in_p2p, on=['slot', 'block_root'], how='left')\n",
    "# combined_data = combined_data.merge(arrival_times[['slot', 'min_arrival_time', 'p50_arrival_time', 'p90_arrival_time', 'p99_arrival_time']], on='slot', how='left')\n",
    "combined_data['num_blobs'] = combined_data['slot'].map(lambda slot: blob_sidecars_map.get(slot, {'num_blobs': 0})['num_blobs'])\n",
    "combined_data['total_blob_size'] = combined_data['slot'].map(lambda slot: blob_sidecars_map.get(slot, {'total_blob_size': 0})['total_blob_size'])\n",
    "\n",
    "# Calculate total data per slot\n",
    "combined_data['total_data_per_slot'] = combined_data['block_total_bytes_compressed'] + combined_data['total_blob_size']\n",
    "combined_data['total_size_mb'] = (combined_data['block_total_bytes_compressed'] + combined_data['total_blob_size']) / 1_000_000  \n",
    "\n",
    "\n",
    "combined_data['block_pct'] = combined_data['block_total_bytes_compressed'] / combined_data['total_data_per_slot'] * 100\n",
    "combined_data['blob_pct'] = combined_data['total_blob_size'] / combined_data['total_data_per_slot'] * 100\n",
    "\n",
    "# Add 'via_mev' column to combined_data\n",
    "combined_data['via_mev'] = combined_data['slot'].isin(mev_slots['slot'])\n",
    "combined_data['date'] = pd.to_datetime(combined_data['time']).dt.date\n",
    "\n",
    "\n",
    "# Create a dataframe with slots and their sizes\n",
    "mev_slots_with_sizes = combined_data[combined_data['via_mev'] == True]\n",
    "non_mev_slots_with_sizes = combined_data[combined_data['via_mev'] == False]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate which event happened first for each slot\n",
    "# combined_data['first_event'] = combined_data.apply(lambda row: min(\n",
    "#     row['first_seen_in_p2p'] or float('inf'),\n",
    "#     row['first_seen_attestation_in_p2p'] or float('inf'), \n",
    "#     row['min_arrival_time'] or float('inf')\n",
    "# ), axis=1)\n",
    "\n",
    "# # Create labels for each event type\n",
    "# combined_data['first_event_type'] = combined_data.apply(lambda row: \n",
    "#     'Block in P2P' if row['first_seen_in_p2p'] == row['first_event'] \n",
    "#     else 'Attestation for Block in P2P' if row['first_seen_attestation_in_p2p'] == row['first_event']\n",
    "#     else 'Block Arrival', axis=1)\n",
    "\n",
    "# # Create 15 slot buckets and get bucket ranges\n",
    "# combined_data['slot_bucket'], bins = pd.qcut(combined_data['slot'], q=15, labels=False, retbins=True)\n",
    "# bucket_ranges = [f'{int(bins[i])}-{int(bins[i+1])}' for i in range(len(bins)-1)]\n",
    "\n",
    "# # Calculate percentages per bucket\n",
    "# bucket_counts = combined_data.groupby(['slot_bucket', 'first_event_type']).size().unstack(fill_value=0)\n",
    "# bucket_percentages = bucket_counts.div(bucket_counts.sum(axis=1), axis=0) * 100\n",
    "\n",
    "# # Create stacked bar chart with improved styling\n",
    "# plt.figure(figsize=(16, 8))\n",
    "# ax = bucket_percentages.plot(\n",
    "#     kind='bar', \n",
    "#     stacked=True, \n",
    "#     color=['#3498db', '#2ecc71', '#e74c3c'],\n",
    "#     width=0.8\n",
    "# )\n",
    "\n",
    "# plt.title('Distribution of First Block Event Type by Slot Range', pad=20, fontsize=16, fontweight='bold')\n",
    "# plt.xlabel('Slot Range', fontsize=14)\n",
    "# plt.ylabel('Percentage', fontsize=14)\n",
    "\n",
    "# # Customize legend\n",
    "# plt.legend(\n",
    "#     title='Event Type',\n",
    "#     title_fontsize=12,\n",
    "#     fontsize=10,\n",
    "#     bbox_to_anchor=(1.05, 1),\n",
    "#     loc='upper left',\n",
    "#     borderaxespad=0\n",
    "# )\n",
    "\n",
    "# # Customize grid\n",
    "# plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "\n",
    "# # Set x-axis labels to slot ranges\n",
    "# plt.xticks(range(len(bucket_ranges)), bucket_ranges, rotation=45, ha='right')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # Print statistics\n",
    "# print(\"\\nAverage Distribution Across Buckets:\")\n",
    "# avg_pct = bucket_percentages.mean()\n",
    "# for event_type, pct in avg_pct.items():\n",
    "#     print(f\"{event_type}: {pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate processing delay by comparing p2p arrival vs block arrival\n",
    "# processing_delay = combined_data[['slot', 'first_seen_in_p2p', 'min_arrival_time']].dropna()\n",
    "# processing_delay['delay'] = processing_delay['min_arrival_time'] - processing_delay['first_seen_in_p2p']\n",
    "\n",
    "# # Filter out negative delays\n",
    "# processing_delay = processing_delay[processing_delay['delay'] >= 0]\n",
    "\n",
    "# # Create 15 slot buckets and get bucket ranges\n",
    "# processing_delay['slot_bucket'], bins = pd.qcut(processing_delay['slot'], q=15, labels=False, retbins=True)\n",
    "# bucket_ranges = [f'{int(bins[i])}-{int(bins[i+1])}' for i in range(len(bins)-1)]\n",
    "\n",
    "# # Calculate median delay and 95th/5th percentiles per bucket\n",
    "# bucket_stats = pd.DataFrame({\n",
    "#     'median': processing_delay.groupby('slot_bucket')['delay'].median(),\n",
    "#     'p98': processing_delay.groupby('slot_bucket')['delay'].apply(lambda x: x.quantile(0.95)),\n",
    "#     'p5': processing_delay.groupby('slot_bucket')['delay'].apply(lambda x: x.quantile(0.05))\n",
    "# })\n",
    "\n",
    "# # Create figure with proper size\n",
    "# plt.figure(figsize=(8, 5.5))\n",
    "\n",
    "# # Plot median and percentiles for each bucket\n",
    "# plt.bar(range(len(bucket_ranges)), bucket_stats['median'], \n",
    "#         alpha=0.7, color='#3498db', label='Median delay')\n",
    "# plt.plot(range(len(bucket_ranges)), bucket_stats['p98'], \n",
    "#          color='#e74c3c', label='95th percentile', linewidth=2)\n",
    "# plt.plot(range(len(bucket_ranges)), bucket_stats['p5'],\n",
    "#          color='#2ecc71', label='5th percentile', linewidth=2)\n",
    "\n",
    "# # Style the plot\n",
    "# plt.title('Block Processing Delay Distribution by Slot Range', pad=20, fontsize=14, fontweight='bold')\n",
    "# plt.xlabel('Slot Range', fontsize=12)\n",
    "# plt.ylabel('Processing Delay (ms)', fontsize=12)\n",
    "# plt.grid(True, alpha=0.3, linestyle='--')\n",
    "# plt.legend(fontsize=10)\n",
    "\n",
    "# # Set x-axis labels to slot ranges\n",
    "# plt.xticks(range(len(bucket_ranges)), bucket_ranges, rotation=45, ha='right')\n",
    "\n",
    "# # Adjust layout to prevent label cutoff\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"Overall median processing delay: {processing_delay['delay'].median():.2f}ms\")\n",
    "# print(f\"Overall 95th percentile delay: {processing_delay['delay'].quantile(0.95):.2f}ms\")\n",
    "# print(f\"Overall 5th percentile delay: {processing_delay['delay'].quantile(0.05):.2f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get head arrival times per client\n",
    "print(\"Getting head arrival times...\")\n",
    "head_arrival_query = text(\"\"\"\n",
    "    SELECT \n",
    "        slot,\n",
    "        MIN(propagation_slot_start_diff) as head_arrival_time,\n",
    "        meta_client_name\n",
    "    FROM beacon_api_eth_v1_events_head FINAL\n",
    "    WHERE \n",
    "        slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "        AND meta_network_name = 'mainnet'\n",
    "    GROUP BY slot, meta_client_name\n",
    "\"\"\")\n",
    "head_arrival_times = pd.DataFrame(\n",
    "    connection.execute(head_arrival_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')}).fetchall(),\n",
    "    columns=['slot', 'head_arrival_time', 'meta_client_name']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get block arrival times per client\n",
    "print(\"Getting block arrival times...\")\n",
    "block_arrival_query = text(\"\"\"\n",
    "    SELECT \n",
    "        slot,\n",
    "        MIN(propagation_slot_start_diff) as block_arrival_time,\n",
    "        meta_client_name\n",
    "    FROM beacon_api_eth_v1_events_block FINAL\n",
    "    WHERE \n",
    "        slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "        AND meta_network_name = 'mainnet'\n",
    "    GROUP BY slot, meta_client_name\n",
    "\"\"\")\n",
    "block_arrival_times = pd.DataFrame(\n",
    "    connection.execute(block_arrival_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')}).fetchall(),\n",
    "    columns=['slot', 'block_arrival_time', 'meta_client_name']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get blob arrival times per client\n",
    "print(\"Getting blob arrival times...\")\n",
    "blob_arrival_query = text(\"\"\"\n",
    "    SELECT\n",
    "        slot,\n",
    "        MAX(propagation_slot_start_diff) as last_blob_arrival_time,\n",
    "        meta_client_name\n",
    "    FROM beacon_api_eth_v1_events_blob_sidecar FINAL\n",
    "    WHERE\n",
    "        slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC') \n",
    "        AND meta_network_name = 'mainnet'\n",
    "    GROUP BY slot, meta_client_name\n",
    "\"\"\")\n",
    "blob_arrival_times = pd.DataFrame(\n",
    "    connection.execute(blob_arrival_query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')}).fetchall(),\n",
    "    columns=['slot', 'last_blob_arrival_time', 'meta_client_name']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all arrival times into one dataframe\n",
    "raw_arrival_data = pd.merge(head_arrival_times, block_arrival_times, on=['slot', 'meta_client_name'], how='outer')\n",
    "raw_arrival_data = pd.merge(raw_arrival_data, blob_arrival_times, on=['slot', 'meta_client_name'], how='outer')\n",
    "\n",
    "# Get the latest arrival time between head/block/blob for each client/slot\n",
    "# Handle missing columns by using fillna\n",
    "raw_arrival_data['arrival_time'] = raw_arrival_data[[\n",
    "    'head_arrival_time', \n",
    "    'block_arrival_time', \n",
    "    'last_blob_arrival_time'\n",
    "]].fillna(-float('inf')).max(axis=1)\n",
    "\n",
    "print(raw_arrival_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add country and group info\n",
    "query = text(\"\"\"\n",
    "    SELECT \n",
    "        meta_client_name,\n",
    "        meta_client_geo_country as country,\n",
    "        CASE\n",
    "            WHEN meta_client_name NOT LIKE 'ethpandaops%' THEN 'home users'\n",
    "            ELSE 'ethpandaops'\n",
    "        END as observed_by_group\n",
    "    FROM beacon_api_eth_v1_events_head FINAL\n",
    "    WHERE\n",
    "        slot_start_date_time BETWEEN toDateTime(:start_date, 'UTC') AND toDateTime(:end_date, 'UTC')\n",
    "        AND meta_client_name != ''\n",
    "        AND meta_client_name IS NOT NULL\n",
    "        AND meta_network_name = 'mainnet'\n",
    "    GROUP BY meta_client_name, meta_client_geo_country\n",
    "\"\"\")\n",
    "\n",
    "client_info = pd.DataFrame(\n",
    "    connection.execute(query, {\"start_date\": start_date.replace('Z', ''), \"end_date\": end_date.replace('Z', '')}).fetchall(),\n",
    "    columns=['meta_client_name', 'country', 'observed_by_group']\n",
    ")\n",
    "\n",
    "print(\"Query done.. merging everything together...\")\n",
    "\n",
    "arrival_data = pd.merge(raw_arrival_data, client_info, on='meta_client_name', how='left')\n",
    "\n",
    "# Filter out very late arrivals and replace -inf with NaN\n",
    "# Subtract 100ms to account for the 100ms delay from the event stream processing.\n",
    "arrival_data['arrival_time'] = arrival_data['arrival_time'].replace(-float('inf'), float('nan')) - 100\n",
    "arrival_data = arrival_data[arrival_data['arrival_time'] < 6000]\n",
    "\n",
    "if len(arrival_data) == 0:\n",
    "    raise ValueError(\"No arrival times found for the given time range\")\n",
    "\n",
    "combined_data = pd.merge(combined_data, arrival_data, on=['slot'], how='left')\n",
    "\n",
    "print(f\"Found arrival times for {len(arrival_data)} slots\")\n",
    "print(arrival_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst_case_block_w_eip_7623=0.72\n",
    "worst_case_block_w_eip_7623_36m=0.864\n",
    "worst_case_block_w_eip_7623_60m=1.43\n",
    "\n",
    "# Calculate p99 block size from data\n",
    "base_size = combined_data['block_total_bytes_compressed'].quantile(0.99) / 1_000_000  # Convert to MB\n",
    "\n",
    "block_sizes = [\n",
    "    {\n",
    "        'label': 'Historical p99 block size',\n",
    "        'value': base_size\n",
    "    },\n",
    "    {\n",
    "        'label': 'Worst case with EIP7623 (30M Gas Limit)',\n",
    "        'value': worst_case_block_w_eip_7623\n",
    "    },\n",
    "    {\n",
    "        'label': 'Worst case with EIP7623 (36M Gas Limit)',\n",
    "        'value': worst_case_block_w_eip_7623_36m\n",
    "    },\n",
    "    {\n",
    "        'label': 'Worst case with EIP7623 (60M Gas Limit)',\n",
    "        'value': worst_case_block_w_eip_7623_60m\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "print(block_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = combined_data['entity'].value_counts().nlargest(10).index.tolist()\n",
    "\n",
    "# entities = ['solo_stakers','kiln']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = (arrival_data\n",
    "    .merge(combined_data[['slot', 'block_total_bytes_compressed', 'total_blob_size', 'entity']], on='slot'))\n",
    "\n",
    "home_users_data = all_data.query(\"observed_by_group == 'home users'\")\n",
    "\n",
    "print(f\"Slots: {len(home_users_data['slot'].unique())}\")\n",
    "print(f\"Arrival events: {len(home_users_data)}\")\n",
    "print(f\"Countries: {len(home_users_data['country'].unique())}\")\n",
    "print(f\"Countries: {', '.join(sorted(home_users_data['country'].unique()))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec, colors\n",
    "\n",
    "# Define color map for title parameters\n",
    "title_param_colors = {\n",
    "    'mev_status': ['#2E86C1', '#E74C3C', '#27AE60'],  \n",
    "    'entity_colors': ['#8E44AD', '#E67E22', '#16A085', '#2980B9', '#C0392B', '#7D3C98', '#F39C12', '#1ABC9C', '#2E86C1', '#E74C3C', '#27AE60', '#8E44AD', '#D35400', '#16A085', '#2980B9'],\n",
    "    'observer_colors': ['#884EA0', '#D35400', '#1ABC9C']\n",
    "}\n",
    "\n",
    "observed_by_groups = ['home users']\n",
    "figures = []\n",
    "trend_data = []\n",
    "\n",
    "# Load logos\n",
    "ethpandaops_logo = plt.imread('./content/ethpandaops.png')\n",
    "xatu_logo = plt.imread('./content/xatu.png')\n",
    "\n",
    "feature_set = {'show_block_size': False, 'show_blob_count': True, 'show_trend_lines': False}\n",
    "\n",
    "# only the top 5 entities\n",
    "top_entities = combined_data['entity'].value_counts().head(5).index.tolist()\n",
    "\n",
    "entities =['solo_stakers']\n",
    "\n",
    "for block_size in block_sizes:\n",
    "    for entity in entities:\n",
    "        for is_mev in [True, False, None]:\n",
    "            for observed_by_group in observed_by_groups:\n",
    "                if observed_by_group == 'nan':\n",
    "                    continue\n",
    "                \n",
    "                print(f\"Creating plot for MEV: {is_mev} {block_size['label']} with {entity} and observed by {observed_by_group}\")\n",
    "                \n",
    "                # Filter data based on conditions\n",
    "                if is_mev is None:\n",
    "                    slots_df = combined_data\n",
    "                elif is_mev:\n",
    "                    slots_df = mev_slots_with_sizes\n",
    "                else:\n",
    "                    slots_df = non_mev_slots_with_sizes\n",
    "                    \n",
    "                # Merge and filter data\n",
    "                plot_data = (arrival_data\n",
    "                    .merge(slots_df[['slot', 'block_total_bytes_compressed', 'total_blob_size', 'entity']], on='slot'))\n",
    "                \n",
    "                if observed_by_group != 'all nodes':\n",
    "                    plot_data = plot_data.query(f\"observed_by_group == '{observed_by_group}'\")\n",
    "                \n",
    "                plot_data = plot_data.query(f\"entity == '{entity}'\")\n",
    "                \n",
    "                valid_mask = ~np.isnan(plot_data['arrival_time'])\n",
    "                n_blocks = len(plot_data['slot'].unique())\n",
    "                n_arrivals = np.sum(valid_mask)\n",
    "\n",
    "                if n_blocks < 100:\n",
    "                    print(f\"Skipping {entity} observed by {observed_by_group} (MEV: {is_mev}) due to low block count: {n_blocks}\")\n",
    "                    continue\n",
    "\n",
    "                if len(plot_data) == 0:\n",
    "                    print(f\"No data for {entity} observed by {observed_by_group}\")\n",
    "                    continue\n",
    "\n",
    "                # Calculate total size in MB\n",
    "                plot_data['total_size_mb'] = (plot_data['block_total_bytes_compressed'] + plot_data['total_blob_size']) / 1_000_000\n",
    "\n",
    "                # Create figure and gridspec\n",
    "                plt.style.use('seaborn-v0_8')\n",
    "                fig = plt.figure(figsize=(14, 10))\n",
    "                \n",
    "                # Store metadata in figure for filename generation\n",
    "                fig.metadata = {\n",
    "                    'block_size': block_size['label'],\n",
    "                    'entity': entity,\n",
    "                    'is_mev': is_mev,\n",
    "                    'observed_by': observed_by_group,\n",
    "                    'features': '_'.join(k for k,v in feature_set.items() if v)\n",
    "                }\n",
    "                \n",
    "                gs = gridspec.GridSpec(2, 1, height_ratios=[1, 10])\n",
    "                \n",
    "                # Create title axes\n",
    "                title_ax = fig.add_subplot(gs[0])\n",
    "                title_ax.axis('off')\n",
    "                \n",
    "                # Create main plot axes\n",
    "                ax = fig.add_subplot(gs[1])\n",
    "\n",
    "                # Calculate square bins based on axis limits\n",
    "                x_max = 3.5  # MB\n",
    "                y_max = 6500  # ms\n",
    "                n_bins = 25  # Number of bins in each dimension\n",
    "                \n",
    "                x_bins = np.linspace(0, x_max, n_bins)\n",
    "                y_bins = np.linspace(0, y_max, n_bins)\n",
    "                \n",
    "                heatmap, xedges, yedges = np.histogram2d(\n",
    "                    plot_data['total_size_mb'],\n",
    "                    plot_data['arrival_time'],\n",
    "                    bins=[x_bins, y_bins]\n",
    "                )\n",
    "                \n",
    "                # Plot heatmap with white to red colormap\n",
    "                im = ax.pcolormesh(xedges, yedges, heatmap.T, \n",
    "                                 cmap='Reds', \n",
    "                                 norm=colors.LogNorm())\n",
    "                cbar = fig.colorbar(im, ax=ax, label='Number of block arrival events')\n",
    "\n",
    "                # Add count text to each cell\n",
    "                for i in range(len(x_bins)-1):\n",
    "                    for j in range(len(y_bins)-1):\n",
    "                        if heatmap[i,j] > 0:  # Only show non-zero counts\n",
    "                            ax.text(x_bins[i] + (x_bins[1]-x_bins[0])/2,\n",
    "                                  y_bins[j] + (y_bins[1]-y_bins[0])/2,\n",
    "                                  int(heatmap[i,j]),\n",
    "                                  ha='center', va='center',\n",
    "                                  color='white' if heatmap[i,j] > np.max(heatmap)/128 else 'black', # Changed from /4 to /8\n",
    "                                  fontsize=8,\n",
    "                                  fontweight='bold')\n",
    "\n",
    "                # Add attestation deadline line\n",
    "                ax.axhline(y=4000, color='black', linestyle='-', alpha=0.8)\n",
    "\n",
    "                mev_text = \"MEV relay blocks\" if is_mev else \"Locally built blocks\" if is_mev is not None else \"All blocks\"\n",
    "                \n",
    "                # Add title text to title axes\n",
    "                title = 'Block Arrival Distribution'\n",
    "                title_ax.text(0.5, 0.6, title, ha='center', va='bottom',\n",
    "                        fontsize=16, color='black', fontweight='bold')\n",
    "                title_ax.text(0.5, 0.3, mev_text, ha='center', va='bottom',\n",
    "                        fontsize=16, color=title_param_colors['mev_status'][0 if is_mev else 1 if is_mev is not None else 2], \n",
    "                        fontweight='bold')\n",
    "                title_ax.text(0.5, 0.0, 'Blocks created by ', ha='right', va='bottom',\n",
    "                        fontsize=16, color='black')\n",
    "                title_ax.text(0.5, 0.0, entity, ha='left', va='bottom',\n",
    "                        fontsize=16, color=title_param_colors['entity_colors'][hash(entity) % len(title_param_colors['entity_colors'])], \n",
    "                        fontweight='bold')\n",
    "                title_ax.text(0.5, -0.3, 'Blocks seen by ', ha='right', va='bottom',\n",
    "                        fontsize=16, color='black')\n",
    "                title_ax.text(0.5, -0.3, observed_by_group, ha='left', va='bottom',\n",
    "                        fontsize=16, color=title_param_colors['observer_colors'][hash(observed_by_group) % len(title_param_colors['observer_colors'])], \n",
    "                        fontweight='bold')\n",
    "\n",
    "                # Add legend for cell values and attestation deadline\n",
    "                legend_text = ('Cell values indicate number of\\nblock arrival events in that region\\n\\n'\n",
    "                             'Black line indicates attestation\\ndeadline at 4s')\n",
    "                ax.text(0.75, 0.95, legend_text,\n",
    "                        ha='left', va='top', transform=ax.transAxes,\n",
    "                        fontsize=10, color='black', style='italic',\n",
    "                        bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'))\n",
    "\n",
    "                # Add logos and text\n",
    "                logo_ax1 = fig.add_axes([0.8, 0.75, 0.1, 0.1]) \n",
    "                logo_ax1.imshow(xatu_logo)\n",
    "                logo_ax1.axis('off')\n",
    "                \n",
    "                text_ax = fig.add_axes([0.8, 0.73, 0.1, 0.02])\n",
    "                text_ax.text(0.5, 0.5, 'Data from Xatu', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "                text_ax.axis('off')\n",
    "\n",
    "                logo_ax2 = fig.add_axes([0.13, 0.75, 0.1, 0.1])\n",
    "                logo_ax2.imshow(ethpandaops_logo)\n",
    "                logo_ax2.axis('off')\n",
    "\n",
    "                text_ax = fig.add_axes([0.13, 0.73, 0.1, 0.02])\n",
    "                text_ax.text(0.5, 0.5, 'ethpandaops.io', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "                text_ax.axis('off')\n",
    "\n",
    "                # Add block and arrival counts\n",
    "                ax.text(0.65, 0.12, f'Total Blocks: {n_blocks:,}\\nArrival events: {n_arrivals:,}', \n",
    "                        ha='left', va='top', transform=ax.transAxes,\n",
    "                        fontsize=12, color='black', \n",
    "                        bbox=dict(facecolor='white', alpha=0.8, edgecolor='gray'))\n",
    "                ax.set_xlabel('Block + Blob Actual Size Compressed (MB)', fontsize=12, fontweight='bold')\n",
    "                ax.set_ylabel('Block Arrival Time (milliseconds)', fontsize=12, fontweight='bold')\n",
    "                ax.grid(True, alpha=0.2, linestyle='--')\n",
    "                ax.set_ylim(0, 6500)\n",
    "                ax.set_xlim(0, 3.5)\n",
    "\n",
    "                plt.subplots_adjust(top=0.85)\n",
    "                plt.show()\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create combined CDF of arrivals for all configs\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# import numpy as np\n",
    "\n",
    "# # Create single CDF plot with all configs\n",
    "# fig = go.Figure()\n",
    "\n",
    "# colors = ['#2E86C1', '#E74C3C', '#28B463', '#F39C12', '#8E44AD', '#17A2B8', '#FD7E14', '#20C997']\n",
    "# color_idx = 0\n",
    "\n",
    "# # Dedupe configs by removing feature_set from uniqueness check\n",
    "# unique_configs = []\n",
    "# seen_configs = set()\n",
    "\n",
    "# for config in configs:\n",
    "#     # Create a key without feature_set for deduplication\n",
    "#     dedup_key = (\n",
    "#         config['block_size']['label'],\n",
    "#         config['entity'],\n",
    "#         config['is_mev'],\n",
    "#         config['observed_by_group']\n",
    "#     )\n",
    "    \n",
    "#     if dedup_key not in seen_configs:\n",
    "#         seen_configs.add(dedup_key)\n",
    "#         unique_configs.append(config)\n",
    "\n",
    "# for config in unique_configs:\n",
    "#     feature_set = config['feature_set']\n",
    "#     block_size = config['block_size']\n",
    "#     entity = config['entity']\n",
    "#     is_mev = config['is_mev']\n",
    "#     observed_by_group = config['observed_by_group']\n",
    "    \n",
    "#     print(f\"Adding CDF for MEV: {is_mev} {block_size['label']} with {entity} and observed by {observed_by_group}\")\n",
    "    \n",
    "#     # Filter data based on conditions\n",
    "#     if is_mev is None:\n",
    "#         slots_df = combined_data\n",
    "#     elif is_mev:\n",
    "#         slots_df = mev_slots_with_sizes\n",
    "#     else:\n",
    "#         slots_df = non_mev_slots_with_sizes\n",
    "        \n",
    "#     # Merge and filter data\n",
    "#     plot_data = (arrival_data\n",
    "#         .merge(slots_df[['slot', 'block_total_bytes_compressed', 'total_blob_size', 'entity']], on='slot'))\n",
    "    \n",
    "#     if observed_by_group != 'all nodes':\n",
    "#         plot_data = plot_data.query(f\"observed_by_group == '{observed_by_group}'\")\n",
    "    \n",
    "#     plot_data = plot_data.query(f\"entity == '{entity}'\")\n",
    "    \n",
    "#     valid_mask = ~np.isnan(plot_data['arrival_time'])\n",
    "#     plot_data = plot_data[valid_mask]\n",
    "    \n",
    "#     n_blocks = len(plot_data['slot'].unique())\n",
    "#     n_arrivals = len(plot_data)\n",
    "\n",
    "#     if n_blocks < 100:\n",
    "#         print(f\"Skipping {entity} observed by {observed_by_group} (MEV: {is_mev}) due to low block count: {n_blocks}\")\n",
    "#         continue\n",
    "\n",
    "#     if len(plot_data) == 0:\n",
    "#         print(f\"No data for {entity} observed by {observed_by_group}\")\n",
    "#         continue\n",
    "\n",
    "#     # Get arrival times for CDF\n",
    "#     arrival_times = plot_data['arrival_time'].values\n",
    "    \n",
    "#     # Calculate CDF values using normalized percentiles for faster computation\n",
    "#     percentiles = np.linspace(0, 100, min(1000, len(arrival_times)))\n",
    "#     x_values = np.percentile(arrival_times, percentiles)\n",
    "#     y_values = percentiles / 100\n",
    "    \n",
    "#     # Create trace name\n",
    "#     mev_text = \"MEV relay\" if is_mev else \"Locally built\" if is_mev is not None else \"All blocks\"\n",
    "#     trace_name = f\"{mev_text} - {entity} - {observed_by_group}\"\n",
    "    \n",
    "#     # Add CDF line\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=x_values,\n",
    "#         y=y_values,\n",
    "#         mode='lines',\n",
    "#         name=trace_name,\n",
    "#         line=dict(color=colors[color_idx % len(colors)], width=2)\n",
    "#     ))\n",
    "    \n",
    "#     color_idx += 1\n",
    "\n",
    "# # Add vertical line at 4000ms (attestation deadline)\n",
    "# fig.add_vline(\n",
    "#     x=4000,\n",
    "#     line_dash=\"dash\",\n",
    "#     line_color=\"red\",\n",
    "#     annotation_text=\"Attestation deadline (4s)\",\n",
    "#     annotation_position=\"top right\"\n",
    "# )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title=\"CDF of Block Arrival Times - All Configurations\",\n",
    "#     xaxis_title=\"Block Arrival Time (milliseconds)\",\n",
    "#     yaxis_title=\"Cumulative Probability\",\n",
    "#     width=1000,\n",
    "#     height=700,\n",
    "#     showlegend=True,\n",
    "#     template=\"plotly_white\",\n",
    "#     legend=dict(\n",
    "#         orientation=\"v\",\n",
    "#         yanchor=\"top\",\n",
    "#         y=1,\n",
    "#         xanchor=\"left\",\n",
    "#         x=1.02\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P66 Head Time Analysis\n",
    "\n",
    "Analyze when 66% of nodes have seen each block (p66 head time) - this represents when a block can be accepted by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate p66 head time for each slot across different node groups\n",
    "# Function to calculate p66 for a group of arrival times\n",
    "def calculate_p66(arrival_times):\n",
    "    if len(arrival_times) == 0:\n",
    "        return np.nan\n",
    "    return np.percentile(arrival_times, 66)\n",
    "\n",
    "# Calculate p66 for each slot and node group\n",
    "p66_results = []\n",
    "\n",
    "for slot in arrival_data['slot'].unique():\n",
    "    slot_data = arrival_data[arrival_data['slot'] == slot]\n",
    "    \n",
    "    # Skip slots with too few observations\n",
    "    if len(slot_data) < 10:\n",
    "        continue\n",
    "    \n",
    "    # Get slot metadata from combined_data\n",
    "    slot_metadata_df = combined_data[combined_data['slot'] == slot]\n",
    "    \n",
    "    # Skip if slot not found in combined_data\n",
    "    if len(slot_metadata_df) == 0:\n",
    "        continue\n",
    "        \n",
    "    slot_metadata = slot_metadata_df.iloc[0]\n",
    "    \n",
    "    # Calculate p66 for all nodes\n",
    "    all_nodes_p66 = calculate_p66(slot_data['arrival_time'].dropna())\n",
    "    \n",
    "    # Calculate p66 for ethpandaops nodes only\n",
    "    ethpandaops_data = slot_data[slot_data['observed_by_group'] == 'ethpandaops']\n",
    "    ethpandaops_p66 = calculate_p66(ethpandaops_data['arrival_time'].dropna())\n",
    "    \n",
    "    # Calculate p66 for home users only\n",
    "    home_users_data = slot_data[slot_data['observed_by_group'] == 'home users']\n",
    "    home_users_p66 = calculate_p66(home_users_data['arrival_time'].dropna())\n",
    "    \n",
    "    # Store results\n",
    "    p66_results.append({\n",
    "        'slot': slot,\n",
    "        'proposer_index': slot_metadata['proposer_index'],\n",
    "        'entity': slot_metadata['entity'],\n",
    "        'via_mev': slot_metadata['via_mev'],\n",
    "        'block_size_mb': slot_metadata['block_total_bytes_compressed'] / 1_000_000,\n",
    "        'total_size_mb': slot_metadata['total_size_mb'],\n",
    "        'num_blobs': slot_metadata['num_blobs'],\n",
    "        'p66_all': all_nodes_p66,\n",
    "        'p66_ethpandaops': ethpandaops_p66,\n",
    "        'p66_homeusers': home_users_p66\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "p66_df = pd.DataFrame(p66_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive CDF plot of p66 head times with Plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Color scheme\n",
    "colors = {\n",
    "    'all': '#2E86C1',\n",
    "    'ethpandaops': '#E74C3C', \n",
    "    'homeusers': '#28B463'\n",
    "}\n",
    "\n",
    "# Nice names for display\n",
    "nice_names = {\n",
    "    'all': 'All nodes',\n",
    "    'ethpandaops': 'ethPandaOps',\n",
    "    'homeusers': 'Home users'\n",
    "}\n",
    "\n",
    "# Line styles for MEV vs non-MEV\n",
    "dash_styles = {\n",
    "    True: 'solid',  # MEV blocks\n",
    "    False: 'dash'  # Non-MEV blocks\n",
    "}\n",
    "\n",
    "# Plot CDFs for different combinations\n",
    "for node_group, color in colors.items():\n",
    "    for is_mev in [True, False]:\n",
    "        # Get the appropriate p66 column\n",
    "        p66_col = f'p66_{node_group}'\n",
    "        \n",
    "        # Filter data\n",
    "        data = p66_df[p66_df['via_mev'] == is_mev][p66_col].dropna()\n",
    "        \n",
    "        if len(data) > 0:\n",
    "            # Sort values for CDF\n",
    "            sorted_data = np.sort(data)\n",
    "            y_values = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            p98 = np.percentile(data, 98)\n",
    "            p95 = np.percentile(data, 95)\n",
    "            median = np.percentile(data, 50)\n",
    "            \n",
    "            # Create label with p99\n",
    "            mev_label = 'MEV' if is_mev else 'Local'\n",
    "            label = f'{mev_label} blocks seen by {nice_names[node_group]} (p98: {p98:.0f}ms)'\n",
    "            \n",
    "            # Add trace\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=sorted_data,\n",
    "                y=y_values,\n",
    "                mode='lines',\n",
    "                name=label,\n",
    "                line=dict(\n",
    "                    color=color,\n",
    "                    width=2.5,\n",
    "                    dash=dash_styles[is_mev]\n",
    "                ),\n",
    "                hovertemplate=f'<b>{nice_names[node_group]} - {mev_label}</b><br>' +\n",
    "                             'P66 Time: %{x:.0f}ms<br>' +\n",
    "                             'Percentile: %{y:.1%}<br>' +\n",
    "                             f'<b>Stats:</b><br>' +\n",
    "                             f'Median: {median:.0f}ms<br>' +\n",
    "                             f'P95: {p95:.0f}ms<br>' +\n",
    "                             f'P98: {p98:.0f}ms' +\n",
    "                             '<extra></extra>'\n",
    "            ))\n",
    "\n",
    "# Add vertical line at 4000ms (attestation deadline)\n",
    "fig.add_vline(\n",
    "    x=4000,\n",
    "    line_dash=\"dot\",\n",
    "    line_color=\"red\",\n",
    "    line_width=2,\n",
    "    annotation_text=\"Attestation deadline (4s)\",\n",
    "    annotation_position=\"top\",\n",
    "    annotation_font_size=12,\n",
    "    annotation_yshift=-150  # Move text to middle of plot\n",
    ")\n",
    "\n",
    "# Calculate overall statistics for annotation\n",
    "stats_text = \"<b>Overall Statistics:</b><br>\"\n",
    "for node_group in ['all', 'ethpandaops', 'homeusers']:\n",
    "    p66_col = f'p66_{node_group}'\n",
    "    median = p66_df[p66_col].median()\n",
    "    p95 = p66_df[p66_col].quantile(0.95)\n",
    "    pct_under_4s = (p66_df[p66_col] < 4000).sum() / len(p66_df[p66_col].dropna()) * 100\n",
    "    stats_text += f\"{nice_names[node_group]}: median={median:.0f}ms, p95={p95:.0f}ms, &lt;4s={pct_under_4s:.1f}%<br>\"\n",
    "\n",
    "# Load logos and convert to base64\n",
    "def image_to_base64(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    buffered = BytesIO()\n",
    "    img.save(buffered, format=\"PNG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "    return f\"data:image/png;base64,{img_str}\"\n",
    "\n",
    "ethpandaops_b64 = image_to_base64('./content/ethpandaops.png')\n",
    "xatu_b64 = image_to_base64('./content/xatu.png')\n",
    "\n",
    "# Update layout with logos\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': 'CDF of P66 Head Times by Node Group and Block Type',\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center',\n",
    "        'font': {'size': 18, 'family': 'Arial Black'}\n",
    "    },\n",
    "    xaxis=dict(\n",
    "        title=\"P66 Head Time (milliseconds)\",\n",
    "        titlefont=dict(size=14, family='Arial Black'),\n",
    "        range=[0, 6000],\n",
    "        gridcolor='rgba(128,128,128,0.2)',\n",
    "        showgrid=True,\n",
    "        showline=True,\n",
    "        linewidth=1,\n",
    "        linecolor='black',\n",
    "        mirror=False,\n",
    "        ticks='outside',\n",
    "        tickmode='linear',\n",
    "        tick0=0,\n",
    "        dtick=1000\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title=\"Cumulative Probability\",\n",
    "        titlefont=dict(size=14, family='Arial Black'),\n",
    "        range=[0, 1],\n",
    "        gridcolor='rgba(128,128,128,0.2)',\n",
    "        showgrid=True,\n",
    "        tickformat='.0%',\n",
    "        showline=True,\n",
    "        linewidth=1,\n",
    "        linecolor='black',\n",
    "        mirror=False,\n",
    "        ticks='outside',\n",
    "        tickmode='linear',\n",
    "        tick0=0,\n",
    "        dtick=0.2\n",
    "    ),\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    width=1200,\n",
    "    height=700,\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        x=0.98,\n",
    "        y=0.02,\n",
    "        xanchor='right',\n",
    "        yanchor='bottom',\n",
    "        bgcolor='rgba(255,255,255,0.9)',\n",
    "        bordercolor='rgba(0,0,0,0.2)',\n",
    "        borderwidth=1,\n",
    "        font=dict(size=11)\n",
    "    ),\n",
    "    margin=dict(t=120)  # Add more top margin for logos\n",
    ")\n",
    "\n",
    "# Add statistics annotation\n",
    "fig.add_annotation(\n",
    "    text=stats_text,\n",
    "    xref=\"paper\",\n",
    "    yref=\"paper\",\n",
    "    x=0.02,\n",
    "    y=0.98,\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"top\",\n",
    "    showarrow=False,\n",
    "    bordercolor=\"rgba(0,0,0,0.2)\",\n",
    "    borderwidth=1,\n",
    "    bgcolor=\"rgba(255,255,255,0.9)\",\n",
    "    font=dict(size=10),\n",
    "    align=\"left\"\n",
    ")\n",
    "\n",
    "# Add ethpandaops logo (bigger)\n",
    "fig.add_layout_image(\n",
    "    dict(\n",
    "        source=ethpandaops_b64,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        x=0.08,\n",
    "        y=1.15,\n",
    "        sizex=0.12,  # Increased from 0.08\n",
    "        sizey=0.12,  # Increased from 0.08\n",
    "        xanchor=\"center\",\n",
    "        yanchor=\"middle\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add Xatu logo\n",
    "fig.add_layout_image(\n",
    "    dict(\n",
    "        source=xatu_b64,\n",
    "        xref=\"paper\",\n",
    "        yref=\"paper\",\n",
    "        x=0.92,\n",
    "        y=1.15,\n",
    "        sizex=0.08,\n",
    "        sizey=0.08,\n",
    "        xanchor=\"center\",\n",
    "        yanchor=\"middle\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add text under logos\n",
    "fig.add_annotation(\n",
    "    text=\"<b>ethpandaops.io</b>\",\n",
    "    xref=\"paper\",\n",
    "    yref=\"paper\",\n",
    "    x=0.08,\n",
    "    y=1.06,\n",
    "    xanchor=\"center\",\n",
    "    yanchor=\"top\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=11, color='#2C3E50')\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=\"<b>Data from Xatu</b>\",\n",
    "    xref=\"paper\",\n",
    "    yref=\"paper\",\n",
    "    x=0.92,\n",
    "    y=1.08,\n",
    "    xanchor=\"center\",\n",
    "    yanchor=\"top\",\n",
    "    showarrow=False,\n",
    "    font=dict(size=11, color='#2C3E50')\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze p66 times by entity with branding\n",
    "# Get top entities by block count\n",
    "top_entities = p66_df['entity'].value_counts().head(10).index.tolist()\n",
    "\n",
    "# Create figure with subplots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "# Create grid with space for title and logos\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 10], hspace=0.05, figure=fig)\n",
    "\n",
    "# Create title/logo axes\n",
    "title_ax = fig.add_subplot(gs[0])\n",
    "title_ax.axis('off')\n",
    "\n",
    "# Create subplot axes for the charts\n",
    "chart_ax = fig.add_subplot(gs[1])\n",
    "chart_ax.axis('off')\n",
    "\n",
    "# Now create subplots within the chart area\n",
    "subfigs = fig.subfigures(1, 1, wspace=0.1, height_ratios=[1])\n",
    "subfig = subfigs\n",
    "axes = subfig.subplots(1, 3)\n",
    "\n",
    "node_groups = ['all', 'ethpandaops', 'homeusers']\n",
    "titles = ['All Nodes', 'EthPandaOps Nodes', 'Home Users']\n",
    "\n",
    "for idx, (node_group, title) in enumerate(zip(node_groups, titles)):\n",
    "    ax = axes[idx]\n",
    "    p66_col = f'p66_{node_group}'\n",
    "    \n",
    "    # Calculate median p66 for each entity\n",
    "    entity_stats = []\n",
    "    for entity in top_entities:\n",
    "        entity_data = p66_df[p66_df['entity'] == entity][p66_col].dropna()\n",
    "        if len(entity_data) > 0:\n",
    "            entity_stats.append({\n",
    "                'entity': entity,\n",
    "                'median_p66': entity_data.median(),\n",
    "                'p25': entity_data.quantile(0.25),\n",
    "                'p75': entity_data.quantile(0.75),\n",
    "                'count': len(entity_data)\n",
    "            })\n",
    "    \n",
    "    entity_stats_df = pd.DataFrame(entity_stats).sort_values('median_p66')\n",
    "    \n",
    "    # Create bar plot with error bars\n",
    "    positions = range(len(entity_stats_df))\n",
    "    ax.bar(positions, entity_stats_df['median_p66'], \n",
    "           yerr=[entity_stats_df['median_p66'] - entity_stats_df['p25'],\n",
    "                 entity_stats_df['p75'] - entity_stats_df['median_p66']],\n",
    "           capsize=5, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "    \n",
    "    # Add attestation deadline line\n",
    "    ax.axhline(y=4000, color='red', linestyle='--', alpha=0.7, \n",
    "               label='Attestation deadline')\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlabel('Entity', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('P66 Head Time (ms)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_xticks(positions)\n",
    "    ax.set_xticklabels(entity_stats_df['entity'], rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add block counts as text\n",
    "    for i, (_, row) in enumerate(entity_stats_df.iterrows()):\n",
    "        ax.text(i, row['median_p66'] + 50, f\"n={row['count']}\", \n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Add title to title axes\n",
    "title_ax.text(0.5, 0.5, 'P66 Head Times by Entity', \n",
    "              ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Load and add logos\n",
    "ethpandaops_logo = plt.imread('./content/ethpandaops.png')\n",
    "xatu_logo = plt.imread('./content/xatu.png')\n",
    "\n",
    "# Add logos to title area\n",
    "logo_size = 0.06\n",
    "# Ethpandaops logo on left\n",
    "logo_ax1 = fig.add_axes([0.05, 0.91, logo_size, logo_size])\n",
    "logo_ax1.imshow(ethpandaops_logo)\n",
    "logo_ax1.axis('off')\n",
    "\n",
    "text_ax1 = fig.add_axes([0.05, 0.89, logo_size, 0.02])\n",
    "text_ax1.text(0.5, 0.5, 'ethpandaops.io', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "text_ax1.axis('off')\n",
    "\n",
    "# Xatu logo on right\n",
    "logo_ax2 = fig.add_axes([0.89, 0.91, logo_size, logo_size])\n",
    "logo_ax2.imshow(xatu_logo)\n",
    "logo_ax2.axis('off')\n",
    "\n",
    "text_ax2 = fig.add_axes([0.89, 0.89, logo_size, 0.02])\n",
    "text_ax2.text(0.5, 0.5, 'Data from Xatu', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "text_ax2.axis('off')\n",
    "\n",
    "plt.subplots_adjust(top=0.88)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between block size and p66 times with branding\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Create grid with space for title and logos\n",
    "gs = gridspec.GridSpec(2, 1, height_ratios=[1, 10], hspace=0.05, figure=fig)\n",
    "\n",
    "# Create title/logo axes\n",
    "title_ax = fig.add_subplot(gs[0])\n",
    "title_ax.axis('off')\n",
    "\n",
    "# Create subplot area\n",
    "subplot_gs = gridspec.GridSpecFromSubplotSpec(1, 3, subplot_spec=gs[1], wspace=0.3)\n",
    "\n",
    "for idx, (node_group, title) in enumerate(zip(['all', 'ethpandaops', 'homeusers'], \n",
    "                                             ['All Nodes', 'EthPandaOps Nodes', 'Home Users'])):\n",
    "    ax = fig.add_subplot(subplot_gs[idx])\n",
    "    p66_col = f'p66_{node_group}'\n",
    "    \n",
    "    # Create scatter plot\n",
    "    valid_data = p66_df[[p66_col, 'total_size_mb']].dropna()\n",
    "    \n",
    "    # Separate MEV and non-MEV blocks\n",
    "    mev_data = valid_data[p66_df['via_mev'] == True]\n",
    "    non_mev_data = valid_data[p66_df['via_mev'] == False]\n",
    "    \n",
    "    # Plot scatter points\n",
    "    ax.scatter(non_mev_data['total_size_mb'], non_mev_data[p66_col], \n",
    "               alpha=0.3, s=10, color='blue', label='Local blocks')\n",
    "    ax.scatter(mev_data['total_size_mb'], mev_data[p66_col], \n",
    "               alpha=0.3, s=10, color='red', label='MEV blocks')\n",
    "    \n",
    "    # Add trend lines\n",
    "    if len(valid_data) > 0:\n",
    "        z = np.polyfit(valid_data['total_size_mb'], valid_data[p66_col], 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.linspace(valid_data['total_size_mb'].min(), \n",
    "                             valid_data['total_size_mb'].max(), 100)\n",
    "        ax.plot(x_trend, p(x_trend), \"k--\", alpha=0.8, linewidth=2, \n",
    "                label=f'Trend: {z[0]:.1f}ms/MB')\n",
    "    \n",
    "    # Add attestation deadline\n",
    "    ax.axhline(y=4000, color='red', linestyle=':', alpha=0.7)\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_xlabel('Total Block + Blob Size (MB)', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('P66 Head Time (ms)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper left', fontsize=10)\n",
    "    ax.set_xlim(0, 3.5)\n",
    "    ax.set_ylim(0, 6000)\n",
    "\n",
    "# Add title to title axes\n",
    "title_ax.text(0.5, 0.5, 'Relationship between Block Size and P66 Head Time', \n",
    "              ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Load and add logos\n",
    "ethpandaops_logo = plt.imread('./content/ethpandaops.png')\n",
    "xatu_logo = plt.imread('./content/xatu.png')\n",
    "\n",
    "# Add logos to title area\n",
    "logo_size = 0.06\n",
    "# Ethpandaops logo on left\n",
    "logo_ax1 = fig.add_axes([0.05, 0.90, logo_size, logo_size])\n",
    "logo_ax1.imshow(ethpandaops_logo)\n",
    "logo_ax1.axis('off')\n",
    "\n",
    "text_ax1 = fig.add_axes([0.05, 0.88, logo_size, 0.02])\n",
    "text_ax1.text(0.5, 0.5, 'ethpandaops.io', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "text_ax1.axis('off')\n",
    "\n",
    "# Xatu logo on right\n",
    "logo_ax2 = fig.add_axes([0.89, 0.90, logo_size, logo_size])\n",
    "logo_ax2.imshow(xatu_logo)\n",
    "logo_ax2.axis('off')\n",
    "\n",
    "text_ax2 = fig.add_axes([0.89, 0.88, logo_size, 0.02])\n",
    "text_ax2.text(0.5, 0.5, 'Data from Xatu', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "text_ax2.axis('off')\n",
    "\n",
    "plt.subplots_adjust(top=0.87)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save p66 data for future analysis\n",
    "p66_df.to_csv('./blob-predictions/p66_head_times.csv', index=False)\n",
    "\n",
    "# Summary statistics\n",
    "stats_text = \"Overall P66 Head Time Statistics:\\n\" + \"=\"*50\n",
    "for node_group in ['all', 'ethpandaops', 'homeusers']:\n",
    "    p66_col = f'p66_{node_group}'\n",
    "    stats_text += f\"\\n\\n{node_group.upper()} NODES:\"\n",
    "    stats_text += f\"\\n  Median: {p66_df[p66_col].median():.0f} ms\"\n",
    "    stats_text += f\"\\n  Mean: {p66_df[p66_col].mean():.0f} ms\"\n",
    "    stats_text += f\"\\n  P25: {p66_df[p66_col].quantile(0.25):.0f} ms\"\n",
    "    stats_text += f\"\\n  P75: {p66_df[p66_col].quantile(0.75):.0f} ms\"\n",
    "    stats_text += f\"\\n  P95: {p66_df[p66_col].quantile(0.95):.0f} ms\"\n",
    "    stats_text += f\"\\n  P99: {p66_df[p66_col].quantile(0.99):.0f} ms\"\n",
    "    \n",
    "    # Percentage meeting attestation deadline\n",
    "    pct_under_4s = (p66_df[p66_col] < 4000).sum() / len(p66_df[p66_col].dropna()) * 100\n",
    "    stats_text += f\"\\n  % under 4s deadline: {pct_under_4s:.1f}%\"\n",
    "\n",
    "# Data contributors summary\n",
    "contributors_text = \"\\n\\nData Contributors Summary:\\n\" + \"=\"*50\n",
    "\n",
    "# Count unique contributors\n",
    "unique_nodes = arrival_data['meta_client_name'].nunique()\n",
    "unique_countries = arrival_data['country'].nunique()\n",
    "\n",
    "# Get country distribution\n",
    "country_counts = arrival_data.groupby('country')['meta_client_name'].nunique().sort_values(ascending=False)\n",
    "top_countries = country_counts.head(10)\n",
    "\n",
    "# Count by node group\n",
    "ethpandaops_nodes = arrival_data[arrival_data['observed_by_group'] == 'ethpandaops']['meta_client_name'].nunique()\n",
    "home_user_nodes = arrival_data[arrival_data['observed_by_group'] == 'home users']['meta_client_name'].nunique()\n",
    "\n",
    "# Get regional distribution (continents based on countries)\n",
    "region_mapping = {\n",
    "    'United States': 'North America',\n",
    "    'Canada': 'North America',\n",
    "    'Mexico': 'North America',\n",
    "    'Germany': 'Europe',\n",
    "    'France': 'Europe',\n",
    "    'United Kingdom': 'Europe',\n",
    "    'Italy': 'Europe',\n",
    "    'Spain': 'Europe',\n",
    "    'Belgium': 'Europe',\n",
    "    'Austria': 'Europe',\n",
    "    'Poland': 'Europe',\n",
    "    'Czechia': 'Europe',\n",
    "    'Croatia': 'Europe',\n",
    "    'Slovenia': 'Europe',\n",
    "    'Hungary': 'Europe',\n",
    "    'Portugal': 'Europe',\n",
    "    'Bulgaria': 'Europe',\n",
    "    'Australia': 'Oceania',\n",
    "    'New Zealand': 'Oceania',\n",
    "    'Brazil': 'South America',\n",
    "    'Singapore': 'Asia',\n",
    "    'India': 'Asia',\n",
    "    'Thailand': 'Asia',\n",
    "    'Kuwait': 'Asia',\n",
    "    'South Africa': 'Africa',\n",
    "    'Nigeria': 'Africa'\n",
    "}\n",
    "\n",
    "arrival_data['region'] = arrival_data['country'].map(region_mapping)\n",
    "unique_regions = arrival_data['region'].nunique()\n",
    "\n",
    "contributors_text += f\"\\n\\nTotal unique nodes: {unique_nodes:,}\"\n",
    "contributors_text += f\"\\n  - EthPandaOps nodes: {ethpandaops_nodes:,}\"\n",
    "contributors_text += f\"\\n  - Home user nodes: {home_user_nodes:,}\"\n",
    "contributors_text += f\"\\n\\nTotal countries: {unique_countries}\"\n",
    "contributors_text += f\"\\nTotal regions: {unique_regions}\"\n",
    "\n",
    "contributors_text += f\"\\n\\nTop 10 countries by node count:\"\n",
    "for country, count in top_countries.items():\n",
    "    contributors_text += f\"\\n  - {country}: {count:,} nodes\"\n",
    "\n",
    "# Region summary\n",
    "region_counts = arrival_data.groupby('region')['meta_client_name'].nunique().sort_values(ascending=False)\n",
    "contributors_text += f\"\\n\\nNodes by region:\"\n",
    "for region, count in region_counts.items():\n",
    "    if pd.notna(region):\n",
    "        contributors_text += f\"\\n  - {region}: {count:,} nodes\"\n",
    "\n",
    "# Total observations\n",
    "total_observations = len(arrival_data)\n",
    "contributors_text += f\"\\n\\nTotal block arrival observations: {total_observations:,}\"\n",
    "contributors_text += f\"\\nUnique slots observed: {arrival_data['slot'].nunique():,}\"\n",
    "\n",
    "print(stats_text)\n",
    "print(contributors_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear and create output directory\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "output_dir = './blob-predictions'\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.makedirs(output_dir)\n",
    "\n",
    "# Save each figure as PNG\n",
    "for i, (size_mb, fig) in enumerate(figures):\n",
    "    metadata = fig.metadata\n",
    "    # Create subdirectory if it doesn't exist\n",
    "    \n",
    "    # Create clean filename components\n",
    "    entity = metadata[\"entity\"].lower().replace(' ', '_')\n",
    "    block_size = str(metadata[\"block_size\"]).replace('.', 'p')\n",
    "    mev_flag = 'mev' if metadata[\"is_mev\"] else 'local' if metadata[\"is_mev\"] is not None else 'all'\n",
    "    observed_by = metadata[\"observed_by\"].lower().replace(' ', '_')\n",
    "    \n",
    "    clean_filename = f'block_arrival_{entity}_{block_size}mb_{mev_flag}_{observed_by}'\n",
    "    filename = os.path.join(output_dir, f'{clean_filename}.png')\n",
    "    \n",
    "    fig.savefig(filename, bbox_inches='tight', dpi=300)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
